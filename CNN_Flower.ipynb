{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(13)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(13)\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, MaxPooling3D, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2             \n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOWER_DAISY_DIR='datasets/flowers/daisy'\n",
    "FLOWER_SUNFLOWER_DIR='datasets/flowers/sunflower'\n",
    "FLOWER_TULIP_DIR='datasets/flowers/tulip'\n",
    "FLOWER_DANDI_DIR='datasets/flowers/dandelion'\n",
    "FLOWER_ROSE_DIR='datasets/flowers/rose'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # FEATURES\n",
    "y = [] # CLASSES\n",
    "\n",
    "IMG_SIZE=150\n",
    "\n",
    "def load_flower_data(DIR, flower_type,):\n",
    "    for img in tqdm(os.listdir(DIR)):\n",
    "        path = os.path.join(DIR, img)\n",
    "        img_read = cv2.imread(path, cv2.IMREAD_COLOR) # INCLUDE COLOR\n",
    "        \n",
    "        if img_read is not None:\n",
    "            img_resized = cv2.resize(img_read, (IMG_SIZE,IMG_SIZE))\n",
    "            X.append(np.array(img_resized))\n",
    "            y.append(str(flower_type))\n",
    "        else:\n",
    "            print(DIR, img)\n",
    "\n",
    "'''\n",
    "datasets/flowers/daisy .ipynb_checkpoints\n",
    "datasets/flowers/dandelion flickr.py\n",
    "datasets/flowers/dandelion flickr.pyc\n",
    "datasets/flowers/dandelion run_me.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_all_data_from_dataset():\n",
    "    load_flower_data(FLOWER_DAISY_DIR, \"Daisy\")\n",
    "    load_flower_data(FLOWER_SUNFLOWER_DIR, \"Sunflower\")\n",
    "    load_flower_data(FLOWER_TULIP_DIR, \"Tulip\")\n",
    "    load_flower_data(FLOWER_DANDI_DIR, \"Dandelion\")\n",
    "    load_flower_data(FLOWER_ROSE_DIR, \"Rose\")\n",
    "    joblib.dump(X, \"X_CLEAN.joblib\")\n",
    "    joblib.dump(y, \"y_CLEAN.joblib\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # FEATURES\n",
    "y = [] # CLASSES\n",
    "\n",
    "def load_saved_dataframes():\n",
    "    X = joblib.load(\"X_CLEAN.joblib\")\n",
    "    y = joblib.load(\"y_CLEAN.joblib\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = init_all_data_from_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_saved_dataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_flowers():\n",
    "    fig, ax = plt.subplots(3,3)\n",
    "    fig.set_size_inches(10,10)\n",
    "    for i in range(3):\n",
    "        for j in range (3):\n",
    "            r = rand.randint(0, len(y))\n",
    "            ax[i,j].set_title(y[r])\n",
    "            RGB_img = cv2.cvtColor(X[r], cv2.COLOR_BGR2RGB)\n",
    "            ax[i,j].imshow(RGB_img)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_flowers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from category_encoders import OneHotEncoder\n",
    "#categorical_encoder = OneHotEncoder(return_df=False, impute_missing=False, handle_unknown=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, y, last_version=True):\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    if last_version:\n",
    "        y = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "    else:\n",
    "        y = categorical_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "    X = X / 255\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.25)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess_data(X, y)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_test_val_split(X, y)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add((Conv2D(32, kernel_size=3, activation='relu', input_shape=(150, 150, 3))))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data_generators_agumented():\n",
    "    \n",
    "    # Get training data, augmented\n",
    "    data_generator_train = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, rotation_range=45, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)\n",
    "    train_generator = data_generator_train.flow_from_directory('datasets/training/', target_size=(150, 150), shuffle=True, seed=13, class_mode='categorical', batch_size=32)\n",
    "  \n",
    "    # Get validation and test data, not augmented\n",
    "    test_val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    validation_generator = test_val_datagen.flow_from_directory('datasets/validation/', target_size=(150, 150), shuffle=True, seed=13, class_mode='categorical', batch_size=32)\n",
    "    test_generator = test_val_datagen.flow_from_directory('datasets/testing/', target_size=(150, 150), shuffle=False, seed=13, class_mode=None, batch_size=1)\n",
    " \n",
    "    return train_generator, validation_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data_generators_not_agumented():\n",
    "    datagen_not_agumented = ImageDataGenerator(rescale=1./255)\n",
    "    train_generator = datagen_not_agumented.flow_from_directory('datasets/training/', target_size=(150, 150), shuffle=True, seed=13, class_mode='categorical', batch_size=32)\n",
    "    validation_generator = datagen_not_agumented.flow_from_directory('datasets/validation/', target_size=(150, 150), shuffle=True, seed=13, class_mode='categorical', batch_size=32)\n",
    "    test_generator = datagen_not_agumented.flow_from_directory('datasets/testing/', target_size=(150, 150), shuffle=False, seed=13, class_mode=None, batch_size=1)\n",
    " \n",
    "    return train_generator, validation_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator, validation_generator, test_generator = get_image_data_generators_not_agumented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples = len(train_generator.classes)\n",
    "validation_samples = len(validation_generator.classes)\n",
    "test_samples = len(test_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=training_samples // epochs, # steps_per_epoch * epochs = training samples\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps= validation_samples // epochs) # validation_steps * epochs = validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_score(history):\n",
    "\n",
    "    print(history.history.keys())\n",
    "    \n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    #plt.savefig(\"model_acc_augmentation_75\")\n",
    "    \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    #plt.savefig(\"model_loss_augmentation_75\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_training_score(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes_manually(X_test, y_test):\n",
    "    y_pred = model.predict_classes(X_test) \n",
    "    y_test = one_hot_encoder.inverse_transform(y_test)\n",
    "    y_test = LabelEncoder().fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes_from_generator():\n",
    "    filenames = test_generator.filenames\n",
    "    nb_samples = len(filenames)\n",
    "    test_generator.reset()\n",
    "    pred = model.predict_generator(test_generator, steps = nb_samples)\n",
    "    predicted_class_indices = np.argmax(pred,axis=1)\n",
    "    return predicted_class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(matrix):\n",
    "    correct_predictions = 0\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            if i == j:\n",
    "                correct_predictions += matrix.item(i, j)\n",
    "    print(\"Accuracy = \", correct_predictions / matrix.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_classes_manually(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_classes_from_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_generator.classes, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "train_dir = 'datasets/training/'\n",
    "validation_dir = 'datasets/validation/'\n",
    "testing_dir = 'datasets/testing/'\n",
    "\n",
    "def extract_features(directory, sample_count, augmented):\n",
    "    \n",
    "    print(\"Extract feature from \", directory)\n",
    "    \n",
    "    features = np.zeros(shape=(sample_count, 3, 3, 2048)) # InceptionV3 = (3, 3, 2048), ResNet50 = (5, 5, 2048) \n",
    "    labels = np.zeros(shape=(sample_count, 5))\n",
    "    \n",
    "    if augmented: \n",
    "        datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, rotation_range=45, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)\n",
    "    else:\n",
    "        datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    generator = datagen.flow_from_directory(directory, target_size=(150,150), batch_size=batch_size, seed=13, class_mode='categorical')\n",
    "    i = 0\n",
    "    \n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = base_model.predict(inputs_batch)\n",
    "        features[i * batch_size : (i+1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i+1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "            \n",
    "    print(\"Done with \", directory)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data augmented/not augmented depending on flag\n",
    "train_features, train_labels = extract_features(train_dir, 2580, augmented=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features, validation_labels = extract_features(validation_dir, 860, augmented=False)\n",
    "test_features, test_labels = extract_features(testing_dir, 860, augmented=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (2580, 3*3*2048))\n",
    "validation_features = np.reshape(validation_features, (860, 3*3*2048))\n",
    "test_features = np.reshape(test_features, (860, 3*3*2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv_base as feature extraction\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=train_features[0].shape))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.0002, rho=0.9, epsilon=None, decay=0.0)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "history = model.fit(train_features, train_labels, epochs=50, batch_size=32, validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=training_samples // epochs, # steps_per_epoch * epochs = training samples\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps= validation_samples // epochs) # validation_steps * epochs = validation samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator, validation_generator, test_generator = get_image_data_generators_agumented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(5, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze all base_model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "training_samples = len(train_generator.classes)\n",
    "validation_samples = len(validation_generator.classes)\n",
    "test_samples = len(test_generator.classes)\n",
    "\n",
    "# Train top layers only\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=training_samples // epochs, # steps_per_epoch * epochs = training samples\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps= validation_samples // epochs) # validation_steps * epochs = validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tuning the top 2 inception blocks alongside the top Dense layers\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=training_samples // epochs, # steps_per_epoch * epochs = training samples\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps= validation_samples // epochs) # validation_steps * epochs = validation samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_training_score(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.argmax(test_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    # Serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"models/\" + name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    # Serialize weights to HDF5\n",
    "    model.save_weights(\"models/\" + name + \".h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "    json_file = open(\"models/\" + name + \".json\", 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"models/\" + name + \".h5\")\n",
    "    model = loaded_model\n",
    "    print(\"Loaded model from disk\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"file_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"file_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators - train, test, val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/keras-team/keras/issues/6452\n",
    "\n",
    "def split_dataset_into_train_test_val_split(all_data_dir, training_data_dir, validation_data_dir, testing_data_dir, split_val):\n",
    "    # Recreate testing and training directories\n",
    "    if testing_data_dir.count('/') > 1:\n",
    "        shutil.rmtree(testing_data_dir, ignore_errors=False)\n",
    "        os.makedirs(testing_data_dir)\n",
    "        print(\"Successfully cleaned directory \" + testing_data_dir)\n",
    "    else:\n",
    "        print(\"Refusing to delete testing data directory \" + testing_data_dir + \" as we prevent you from doing stupid things!\")\n",
    "        \n",
    "    if validation_data_dir.count('/') > 1:\n",
    "        shutil.rmtree(training_data_dir, ignore_errors=False)\n",
    "        os.makedirs(training_data_dir)\n",
    "        print(\"Successfully cleaned directory \" + validation_data_dir)\n",
    "    else:\n",
    "        print(\"Refusing to delete validation data directory \" + validation_data_dir + \" as we prevent you from doing stupid things!\")\n",
    "\n",
    "    if training_data_dir.count('/') > 1:\n",
    "        shutil.rmtree(training_data_dir, ignore_errors=False)\n",
    "        os.makedirs(training_data_dir)\n",
    "        print(\"Successfully cleaned directory \" + training_data_dir)\n",
    "    else:\n",
    "        print(\"Refusing to delete testing data directory \" + training_data_dir + \" as we prevent you from doing stupid things!\")\n",
    "        \n",
    "    num_training_files = 0\n",
    "    num_validation_files = 0\n",
    "    num_testing_files = 0\n",
    "\n",
    "    for subdir, dirs, files in os.walk(all_data_dir):\n",
    "        category_name = os.path.basename(subdir)\n",
    "\n",
    "        # Don't create a subdirectory for the root directory\n",
    "        print(category_name + \" vs \" + os.path.basename(all_data_dir))\n",
    "        if category_name == os.path.basename(all_data_dir):\n",
    "            continue\n",
    "\n",
    "        training_data_category_dir = training_data_dir + '/' + category_name\n",
    "        validation_data_category_dir = validation_data_dir + '/' + category_name\n",
    "        testing_data_category_dir = testing_data_dir + '/' + category_name\n",
    "\n",
    "        if not os.path.exists(training_data_category_dir):\n",
    "            os.mkdir(training_data_category_dir)\n",
    "        \n",
    "        if not os.path.exists(validation_data_category_dir):\n",
    "            os.mkdir(validation_data_category_dir)\n",
    "        \n",
    "        if not os.path.exists(testing_data_category_dir):\n",
    "            os.mkdir(testing_data_category_dir)\n",
    "\n",
    "        for file in files:\n",
    "            input_file = os.path.join(subdir, file)\n",
    "            rand = np.random.rand(1)\n",
    "            if  rand < split_val:\n",
    "                shutil.copy(input_file, testing_data_dir + '/' + category_name + '/' + file)\n",
    "                num_testing_files += 1\n",
    "            elif rand > split_val and rand < split_val*2:\n",
    "                shutil.copy(input_file, validation_data_dir + '/' + category_name + '/' + file)\n",
    "                num_validation_files += 1\n",
    "            else:\n",
    "                shutil.copy(input_file, training_data_dir + '/' + category_name + '/' + file)\n",
    "                num_training_files += 1\n",
    "\n",
    "    print(\"Processed \" + str(num_training_files) + \" training files.\")\n",
    "    print(\"Processed \" + str(num_validation_files) + \" validation files.\")\n",
    "    print(\"Processed \" + str(num_testing_files) + \" testing files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset_into_train_test_val_split('datasets/flowers/', 'datasets/training', 'datasets/validation', 'datasets/testing', 0.20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 309,
   "position": {
    "height": "438.965px",
    "left": "1284.96px",
    "right": "20px",
    "top": "55.9609px",
    "width": "292.969px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
