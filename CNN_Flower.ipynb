{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(13)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(13)\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, MaxPooling3D, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2             \n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOWER_DAISY_DIR='datasets/flowers/daisy'\n",
    "FLOWER_SUNFLOWER_DIR='datasets/flowers/sunflower'\n",
    "FLOWER_TULIP_DIR='datasets/flowers/tulip'\n",
    "FLOWER_DANDI_DIR='datasets/flowers/dandelion'\n",
    "FLOWER_ROSE_DIR='datasets/flowers/rose'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # FEATURES\n",
    "y = [] # CLASSES\n",
    "\n",
    "IMG_SIZE=150\n",
    "\n",
    "def load_flower_data(DIR, flower_type,):\n",
    "    for img in tqdm(os.listdir(DIR)):\n",
    "        path = os.path.join(DIR, img)\n",
    "        img_read = cv2.imread(path, cv2.IMREAD_COLOR) # INCLUDE COLOR\n",
    "        \n",
    "        if img_read is not None:\n",
    "            img_resized = cv2.resize(img_read, (IMG_SIZE,IMG_SIZE))\n",
    "            X.append(np.array(img_resized))\n",
    "            y.append(str(flower_type))\n",
    "        else:\n",
    "            print(DIR, img)\n",
    "\n",
    "'''\n",
    "datasets/flowers/daisy .ipynb_checkpoints\n",
    "datasets/flowers/dandelion flickr.py\n",
    "datasets/flowers/dandelion flickr.pyc\n",
    "datasets/flowers/dandelion run_me.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_all_data_from_dataset():\n",
    "    load_flower_data(FLOWER_DAISY_DIR, \"Daisy\")\n",
    "    load_flower_data(FLOWER_SUNFLOWER_DIR, \"Sunflower\")\n",
    "    load_flower_data(FLOWER_TULIP_DIR, \"Tulip\")\n",
    "    load_flower_data(FLOWER_DANDI_DIR, \"Dandelion\")\n",
    "    load_flower_data(FLOWER_ROSE_DIR, \"Rose\")\n",
    "    joblib.dump(X, \"X_CLEAN.joblib\")\n",
    "    joblib.dump(y, \"y_CLEAN.joblib\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # FEATURES\n",
    "y = [] # CLASSES\n",
    "\n",
    "def load_saved_dataframes():\n",
    "    X = joblib.load(\"X_CLEAN.joblib\")\n",
    "    y = joblib.load(\"y_CLEAN.joblib\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = init_all_data_from_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_saved_dataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_flowers():\n",
    "    fig, ax = plt.subplots(3,3)\n",
    "    fig.set_size_inches(10,10)\n",
    "    for i in range(3):\n",
    "        for j in range (3):\n",
    "            r = rand.randint(0, len(y))\n",
    "            ax[i,j].set_title(y[r])\n",
    "            RGB_img = cv2.cvtColor(X[r], cv2.COLOR_BGR2RGB)\n",
    "            ax[i,j].imshow(RGB_img)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_flowers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from category_encoders import OneHotEncoder\n",
    "#categorical_encoder = OneHotEncoder(return_df=False, impute_missing=False, handle_unknown=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, y, last_version=True):\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    if last_version:\n",
    "        y = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "    else:\n",
    "        y = categorical_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "    X = X / 255\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.25)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess_data(X, y)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_test_val_split(X, y)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add((Conv2D(32, kernel_size=3, activation='relu', input_shape=(150, 150, 3))))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 3,455,173\n",
      "Trainable params: 3,455,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data_generators_agumented():\n",
    "    \n",
    "    # Get training data, augmented\n",
    "    data_generator_train = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, rotation_range=45, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)\n",
    "    train_generator = data_generator_train.flow_from_directory('datasets/training/', target_size=(150, 150), shuffle=True, seed=13, class_mode='categorical', batch_size=32)\n",
    "  \n",
    "    # Get validation and test data, not augmented\n",
    "    test_val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    validation_generator = test_val_datagen.flow_from_directory('datasets/validation/', target_size=(150, 150), shuffle=True, seed=13, class_mode='categorical', batch_size=32)\n",
    "    test_generator = test_val_datagen.flow_from_directory('datasets/testing/', target_size=(150, 150), shuffle=False, seed=13, class_mode=None, batch_size=1)\n",
    " \n",
    "    return train_generator, validation_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data_generators_not_agumented():\n",
    "    datagen_not_agumented = ImageDataGenerator(rescale=1./255)\n",
    "    train_generator = datagen_not_agumented.flow_from_directory('datasets/training/', target_size=(150, 150), shuffle=True, seed=13, class_mode='categorical', batch_size=32)\n",
    "    validation_generator = datagen_not_agumented.flow_from_directory('datasets/validation/', target_size=(150, 150), shuffle=True, seed=13, class_mode='categorical', batch_size=32)\n",
    "    test_generator = datagen_not_agumented.flow_from_directory('datasets/testing/', target_size=(150, 150), shuffle=False, seed=13, class_mode=None, batch_size=1)\n",
    " \n",
    "    return train_generator, validation_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2580 images belonging to 5 classes.\n",
      "Found 860 images belonging to 5 classes.\n",
      "Found 860 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, validation_generator, test_generator = get_image_data_generators_agumented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples = len(train_generator.classes)\n",
    "validation_samples = len(validation_generator.classes)\n",
    "test_samples = len(test_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=training_samples // epochs, # steps_per_epoch * epochs = training samples\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps= validation_samples // epochs) # validation_steps * epochs = validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_score(history):\n",
    "\n",
    "    print(history.history.keys())\n",
    "    \n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    #plt.savefig(\"model_acc_augmentation_75\")\n",
    "    \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    #plt.savefig(\"model_loss_augmentation_75\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_training_score(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes_manually(X_test, y_test):\n",
    "    y_pred = model.predict_classes(X_test) \n",
    "    y_test = one_hot_encoder.inverse_transform(y_test)\n",
    "    y_test = LabelEncoder().fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes_from_generator():\n",
    "    filenames = test_generator.filenames\n",
    "    nb_samples = len(filenames)\n",
    "    test_generator.reset()\n",
    "    pred = model.predict_generator(test_generator, steps = nb_samples)\n",
    "    predicted_class_indices = np.argmax(pred,axis=1)\n",
    "    return predicted_class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(matrix):\n",
    "    correct_predictions = 0\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            if i == j:\n",
    "                correct_predictions += matrix.item(i, j)\n",
    "    print(\"Accuracy = \", correct_predictions / matrix.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_classes_manually(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_classes_from_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_generator.classes, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0.69\n",
    "\n",
    "array([[104,  25,   5,  16,  18],\n",
    "       [  4, 156,   1,  30,  15],\n",
    "       [  4,  15,  60,  13,  65],\n",
    "       [  4,   6,   1, 115,  10],\n",
    "       [  2,   7,  22,  13, 149]])\n",
    "       \n",
    "0.76       \n",
    "array([[125,  12,  10,  10,  11],\n",
    "       [ 12, 164,   7,  18,   5],\n",
    "       [ 12,   5, 113,   8,  19],\n",
    "       [  3,   5,   1, 124,   3],\n",
    "       [  3,   5,  41,  11, 133]])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[125,  12,  10,  10,  11],\n",
       "       [ 12, 164,   7,  18,   5],\n",
       "       [ 12,   5, 113,   8,  19],\n",
       "       [  3,   5,   1, 124,   3],\n",
       "       [  3,   5,  41,  11, 133]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.7662790697674419\n"
     ]
    }
   ],
   "source": [
    "accuracy(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "train_dir = 'datasets/training/'\n",
    "validation_dir = 'datasets/validation/'\n",
    "testing_dir = 'datasets/testing/'\n",
    "\n",
    "def extract_features(directory, sample_count, augmented):\n",
    "    \n",
    "    print(\"Extract feature from \", directory)\n",
    "    \n",
    "    features = np.zeros(shape=(sample_count, 3, 3, 2048)) # InceptionV3 = (3, 3, 2048), ResNet50 = (5, 5, 2048) \n",
    "    labels = np.zeros(shape=(sample_count, 5))\n",
    "    \n",
    "    if augmented: \n",
    "        datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, rotation_range=45, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)\n",
    "    else:\n",
    "        datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    generator = datagen.flow_from_directory(directory, target_size=(150,150), batch_size=batch_size, seed=13, class_mode='categorical')\n",
    "    i = 0\n",
    "    \n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = base_model.predict(inputs_batch)\n",
    "        features[i * batch_size : (i+1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i+1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "            \n",
    "    print(\"Done with \", directory)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract feature from  datasets/training/\n",
      "Found 2580 images belonging to 5 classes.\n",
      "Done with  datasets/training/\n"
     ]
    }
   ],
   "source": [
    "# Training data augmented/not augmented depending on flag\n",
    "train_features, train_labels = extract_features(train_dir, 2580, augmented=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract feature from  datasets/validation/\n",
      "Found 860 images belonging to 5 classes.\n",
      "Done with  datasets/validation/\n",
      "Extract feature from  datasets/testing/\n",
      "Found 860 images belonging to 5 classes.\n",
      "Done with  datasets/testing/\n"
     ]
    }
   ],
   "source": [
    "validation_features, validation_labels = extract_features(validation_dir, 860, augmented=False)\n",
    "test_features, test_labels = extract_features(testing_dir, 860, augmented=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (2580, 3*3*2048))\n",
    "validation_features = np.reshape(validation_features, (860, 3*3*2048))\n",
    "test_features = np.reshape(test_features, (860, 3*3*2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv_base as feature extraction\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=train_features[0].shape))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 512)               9437696   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 9,570,309\n",
      "Trainable params: 9,570,309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.0002, rho=0.9, epsilon=None, decay=0.0)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "history = model.fit(train_features, train_labels, epochs=50, batch_size=32, validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=training_samples // epochs, # steps_per_epoch * epochs = training samples\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps= validation_samples // epochs) # validation_steps * epochs = validation samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator, validation_generator, test_generator = get_image_data_generators_agumented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(5, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze all base_model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "training_samples = len(train_generator.classes)\n",
    "validation_samples = len(validation_generator.classes)\n",
    "test_samples = len(test_generator.classes)\n",
    "\n",
    "# Train top layers only\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=training_samples // epochs, # steps_per_epoch * epochs = training samples\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps= validation_samples // epochs) # validation_steps * epochs = validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tuning the top 2 inception blocks alongside the top Dense layers\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=training_samples // epochs, # steps_per_epoch * epochs = training samples\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps= validation_samples // epochs) # validation_steps * epochs = validation samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_training_score(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.argmax(test_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[142,   4,   7,   9,   6],\n",
       "       [ 11, 171,   6,  12,   6],\n",
       "       [  2,   2, 129,   8,  16],\n",
       "       [  5,   3,   5, 116,   7],\n",
       "       [  1,   2,  21,  10, 159]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(test_labels, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.8337209302325581\n"
     ]
    }
   ],
   "source": [
    "accuracy(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_classes_from_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_generator.classes, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    # Serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"models/\" + name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    # Serialize weights to HDF5\n",
    "    model.save_weights(\"models/\" + name + \".h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "    json_file = open(\"models/\" + name + \".json\", 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"models/\" + name + \".h5\")\n",
    "    model = loaded_model\n",
    "    print(\"Loaded model from disk\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"inception_V3_83\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators - train, test, val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/keras-team/keras/issues/6452\n",
    "\n",
    "def split_dataset_into_train_test_val_split(all_data_dir, training_data_dir, validation_data_dir, testing_data_dir, split_val):\n",
    "    # Recreate testing and training directories\n",
    "    if testing_data_dir.count('/') > 1:\n",
    "        shutil.rmtree(testing_data_dir, ignore_errors=False)\n",
    "        os.makedirs(testing_data_dir)\n",
    "        print(\"Successfully cleaned directory \" + testing_data_dir)\n",
    "    else:\n",
    "        print(\"Refusing to delete testing data directory \" + testing_data_dir + \" as we prevent you from doing stupid things!\")\n",
    "        \n",
    "    if validation_data_dir.count('/') > 1:\n",
    "        shutil.rmtree(training_data_dir, ignore_errors=False)\n",
    "        os.makedirs(training_data_dir)\n",
    "        print(\"Successfully cleaned directory \" + validation_data_dir)\n",
    "    else:\n",
    "        print(\"Refusing to delete validation data directory \" + validation_data_dir + \" as we prevent you from doing stupid things!\")\n",
    "\n",
    "    if training_data_dir.count('/') > 1:\n",
    "        shutil.rmtree(training_data_dir, ignore_errors=False)\n",
    "        os.makedirs(training_data_dir)\n",
    "        print(\"Successfully cleaned directory \" + training_data_dir)\n",
    "    else:\n",
    "        print(\"Refusing to delete testing data directory \" + training_data_dir + \" as we prevent you from doing stupid things!\")\n",
    "        \n",
    "    num_training_files = 0\n",
    "    num_validation_files = 0\n",
    "    num_testing_files = 0\n",
    "\n",
    "    for subdir, dirs, files in os.walk(all_data_dir):\n",
    "        category_name = os.path.basename(subdir)\n",
    "\n",
    "        # Don't create a subdirectory for the root directory\n",
    "        print(category_name + \" vs \" + os.path.basename(all_data_dir))\n",
    "        if category_name == os.path.basename(all_data_dir):\n",
    "            continue\n",
    "\n",
    "        training_data_category_dir = training_data_dir + '/' + category_name\n",
    "        validation_data_category_dir = validation_data_dir + '/' + category_name\n",
    "        testing_data_category_dir = testing_data_dir + '/' + category_name\n",
    "\n",
    "        if not os.path.exists(training_data_category_dir):\n",
    "            os.mkdir(training_data_category_dir)\n",
    "        \n",
    "        if not os.path.exists(validation_data_category_dir):\n",
    "            os.mkdir(validation_data_category_dir)\n",
    "        \n",
    "        if not os.path.exists(testing_data_category_dir):\n",
    "            os.mkdir(testing_data_category_dir)\n",
    "\n",
    "        for file in files:\n",
    "            input_file = os.path.join(subdir, file)\n",
    "            rand = np.random.rand(1)\n",
    "            if  rand < split_val:\n",
    "                shutil.copy(input_file, testing_data_dir + '/' + category_name + '/' + file)\n",
    "                num_testing_files += 1\n",
    "            elif rand > split_val and rand < split_val*2:\n",
    "                shutil.copy(input_file, validation_data_dir + '/' + category_name + '/' + file)\n",
    "                num_validation_files += 1\n",
    "            else:\n",
    "                shutil.copy(input_file, training_data_dir + '/' + category_name + '/' + file)\n",
    "                num_training_files += 1\n",
    "\n",
    "    print(\"Processed \" + str(num_training_files) + \" training files.\")\n",
    "    print(\"Processed \" + str(num_validation_files) + \" validation files.\")\n",
    "    print(\"Processed \" + str(num_testing_files) + \" testing files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset_into_train_test_val_split('datasets/flowers/', 'datasets/training', 'datasets/validation', 'datasets/testing', 0.20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 309,
   "position": {
    "height": "438.965px",
    "left": "1284.96px",
    "right": "20px",
    "top": "55.9609px",
    "width": "292.969px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
