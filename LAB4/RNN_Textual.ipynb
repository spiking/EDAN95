{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy.random import seed\n",
    "seed(13)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(13)\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, MaxPooling3D, Dropout, Embedding, Bidirectional, SimpleRNN, LSTM, GRU\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json \n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tempfile import TemporaryFile\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2             \n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '-DOCSTART- -X- -X- O'}]\n",
      "{'id': '-DOCSTART- -X- -X- O'}\n",
      "<class '__main__.Token'>\n",
      "[{'id': 'EU NNP B-NP B-ORG'}, {'id': 'rejects VBZ B-VP O'}, {'id': 'German JJ B-NP B-MISC'}, {'id': 'call NN I-NP O'}, {'id': 'to TO B-VP O'}, {'id': 'boycott VB I-VP O'}, {'id': 'British JJ B-NP B-MISC'}, {'id': 'lamb NN I-NP O'}, {'id': '. . O O'}]\n",
      "La\n",
      "True\n",
      "{'pos', 'lemma', 'cpos', 'feats', 'form', 'id'}\n"
     ]
    }
   ],
   "source": [
    "# %load conll_dictorizer.py\n",
    "\"\"\"\n",
    "CoNLL 2009 file readers and writers for the parts of speech.\n",
    "Version with a class modeled as a vectorizer\n",
    "\"\"\"\n",
    "__author__ = \"Pierre Nugues\"\n",
    "\n",
    "import regex as re\n",
    "\n",
    "\n",
    "def save(file, corpus_dict, column_names):\n",
    "    \"\"\"\n",
    "    Saves the corpus in a file\n",
    "    :param file:\n",
    "    :param corpus_dict:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(file, 'w') as f_out:\n",
    "        for sentence in corpus_dict:\n",
    "            sentence_lst = []\n",
    "            for row in sentence:\n",
    "                items = map(lambda x: row.get(x, '_'), column_names)\n",
    "                sentence_lst += '\\t'.join(items) + '\\n'\n",
    "            sentence_lst += '\\n'\n",
    "            f_out.write(''.join(sentence_lst))\n",
    "\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    BASE = os.getcwd()\n",
    "    train_file = os.path.join(BASE, 'datasets/train.txt')\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'cpos', 'pos', 'feats']\n",
    "    train = open(train_file).read().strip()\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "    train_dict = conll_dict.transform(train)\n",
    "\n",
    "    print(train_dict[0])\n",
    "    print(train_dict[0][0])\n",
    "    print(type(train_dict[0][0]))\n",
    "    #print(train_dict[0][0]['form'])\n",
    "    print(train_dict[1])\n",
    "    tok = Token({'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'})\n",
    "    print(tok['form'])\n",
    "    print('form' in tok)\n",
    "\n",
    "    save('out', train_dict, column_names)\n",
    "\n",
    "    tok_dict = {'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'}\n",
    "    tok_dict2 = {'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'}\n",
    "\n",
    "    tok_set = set(tok_dict)\n",
    "    print(tok_set)\n",
    "\n",
    "    tok_set = tok_set.union(tok_dict2)\n",
    "    #print(tok_set)\n",
    "\n",
    "    #print(tok.keys())\n",
    "\n",
    "    # exit()\n",
    "    word_set = set()\n",
    "    word_set = set(tok_dict.values())\n",
    "    #print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    word_set = set(tok.values())\n",
    "    #print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    word_set.update(tok.values())\n",
    "    #print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    #print(\"Token value:\", tok.values())\n",
    "    word_set = word_set.union(set(tok.values()))\n",
    "    #print(list(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'form': '-DOCSTART-', 'ppos': '-X-', 'pchunk': '-X-', 'ner': 'O'}]\n",
      "[{'form': 'EU', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-ORG'}, {'form': 'rejects', 'ppos': 'VBZ', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'German', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'call', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'boycott', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'British', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}]\n"
     ]
    }
   ],
   "source": [
    "# %load datasets.py\n",
    "from conll_dictorizer import CoNLLDictorizer, Token\n",
    "import os\n",
    "\n",
    "def load_conll2009_pos():\n",
    "    train_file = 'datasets\\train.txt'\n",
    "    dev_file = 'datasets\\valid.txt'\n",
    "    test_file = 'datasets\\test.txt'\n",
    "    test2_file = 'simple_pos_test.txt'\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'plemma', 'pos', 'ppos']\n",
    "\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "def load_conll2003_en():\n",
    "    BASE_DIR = os.getcwd()\n",
    "    train_file = BASE_DIR + '/datasets/train.txt'\n",
    "    dev_file = BASE_DIR + '/datasets/valid.txt'\n",
    "    test_file = BASE_DIR + '/datasets/test.txt'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "    train_dict = conll_dict.transform(train_sentences)\n",
    "    val_dict = conll_dict.transform(dev_sentences)\n",
    "    test_dict = conll_dict.transform(test_sentences)\n",
    "    print(train_dict[0])\n",
    "    print(train_dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(file):\n",
    "    embeddings_dict = {}\n",
    "    glove = open(file, encoding='utf-8')\n",
    "    \n",
    "    for line in glove:\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding_vec_word = np.array(line[1:], dtype='float32')\n",
    "        embeddings_dict[word] = embedding_vec_word\n",
    "        \n",
    "    glove.close()\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_name):\n",
    "    with open('files/' + file_name + '.pkl', 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(file_name, file):\n",
    "    with open('files/' + file_name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = load_file('embeddings_dict')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = load_file('X')\n",
    "y = load_file('y')\n",
    "X_indices_to_words = load_file('X_indices_to_words')\n",
    "y_indices_to_len = load_file('y_indices_to_len')\n",
    "X_words_to_indices = load_file('X_words_to_indices')\n",
    "y_len_to_indices = load_file('y_len_to_indices')\n",
    "X_vocabulary = load_file('X_vocabulary')\n",
    "y_vocabulary = load_file('X_vocabulary')\n",
    "X_only_indices = load_file('X_only_indices')\n",
    "y_only_indices = load_file('y_only_indices')\n",
    "word_embedding_matrix = load_file('word_embedding_matrix')\n",
    "embeddings_dict = load_file('embeddings_dict')\n",
    "cs_most_similar = load_file('cs_most_similar')\n",
    "X_train = load_file('X_train')\n",
    "y_train = load_file('y_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_dict is a list of lists of dictionaries\n",
    "def extract_features(train_dict):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for sentence in train_dict:\n",
    "        X_sentence = []\n",
    "        y_sentence = []\n",
    "        for word in sentence:\n",
    "            w = word['form'].lower()\n",
    "            n = word['ner']\n",
    "            X_sentence.append(w)\n",
    "            y_sentence.append(n)\n",
    "    \n",
    "        X.append(X_sentence)\n",
    "        y.append(y_sentence)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract words and ner tags - X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence words:  ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "Sentence NER:  ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "X, y = extract_features(train_dict)\n",
    "print('Sentence words: ', X[1])\n",
    "print('Sentence NER: ', y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(X, WORDS=True):\n",
    "    X_vocabulary = set()\n",
    "    \n",
    "    if WORDS:\n",
    "        X_vocabulary.add(\"UNKNOWN_WORD\")\n",
    "        \n",
    "    for sentence in X:\n",
    "        for word in sentence:\n",
    "            X_vocabulary.add(word)\n",
    "    \n",
    "    return sorted(list(X_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size WORDS:  21011\n"
     ]
    }
   ],
   "source": [
    "X_vocabulary = create_vocabulary(X, WORDS=True)\n",
    "print(\"Vocabulary size WORDS: \", len(X_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size NER:  9\n"
     ]
    }
   ],
   "source": [
    "y_vocabulary = create_vocabulary(y, WORDS=False)\n",
    "print(\"Vocabulary size NER: \", len(y_vocabulary))\n",
    "nbr_of_classes = len(y_vocabulary) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add words from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in the vocabulary total, X_vocabulary: 402596\n"
     ]
    }
   ],
   "source": [
    "for word in embeddings_dict.keys():\n",
    "    X_vocabulary.append(word)\n",
    "\n",
    "X_vocabulary = sorted(list(set(X_vocabulary)))\n",
    "total_word_count = len(X_vocabulary)\n",
    "print('Words in the vocabulary total, X_vocabulary:', total_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create indices and inverted indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices(X):\n",
    "    return dict(enumerate(X), start=2) # 0 is padding, 1 is unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "X_indices_to_words = {}\n",
    "\n",
    "for w in X_vocabulary:\n",
    "    X_indices_to_words[i] = w\n",
    "    i += 1\n",
    "\n",
    "y_indices_to_len = {}\n",
    "y_indices_to_len[0] = 'O' #PADDING\n",
    "y_indices_to_len[1] = 'UNKNOWN_WORD'\n",
    "i = 2\n",
    "\n",
    "for l in y_vocabulary:\n",
    "    if l != 'O':\n",
    "        y_indices_to_len[i] = l\n",
    "        i += 1\n",
    "        \n",
    "#X_indices_to_len = dict(enumerate(X_vocabulary), start=2)\n",
    "#y_indices_to_len = dict(enumerate(y_vocabulary), start=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'UNKNOWN_WORD',\n",
       " 2: 'B-LOC',\n",
       " 3: 'B-MISC',\n",
       " 4: 'B-ORG',\n",
       " 5: 'B-PER',\n",
       " 6: 'I-LOC',\n",
       " 7: 'I-MISC',\n",
       " 8: 'I-ORG',\n",
       " 9: 'I-PER'}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_indices_to_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_indices(X):\n",
    "    return {v: k for k, v in X.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words_to_indices = create_inverted_indices(X_indices_to_words)\n",
    "y_len_to_indices = create_inverted_indices(y_indices_to_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'UNKNOWN_WORD': 1,\n",
       " 'B-LOC': 2,\n",
       " 'B-MISC': 3,\n",
       " 'B-ORG': 4,\n",
       " 'B-PER': 5,\n",
       " 'I-LOC': 6,\n",
       " 'I-MISC': 7,\n",
       " 'I-ORG': 8,\n",
       " 'I-PER': 9}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_len_to_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index: [('!', 2), ('!!', 3), ('!!!', 4)]\n",
      "LEN index: [('O', 0), ('UNKNOWN_WORD', 1), ('B-LOC', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Word index:', list(X_words_to_indices.items())[:3])\n",
    "print('LEN index:', list(y_len_to_indices.items())[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode lists - Convert to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_to_indices(X, X_words_to_indices, num_words=None):\n",
    "    X_encoded = []\n",
    "    for x in X:\n",
    "        X_encoded_words = []\n",
    "        if num_words:\n",
    "            # We map the unknown words to the second first index of the matrix, for the test set\n",
    "            X_encoded_words = list(map(lambda x: X_words_to_indices.get(x,1), x))\n",
    "        else:\n",
    "             X_encoded_words = list(map(X_words_to_indices.get, x))\n",
    "            #for val in x:\n",
    "            #    X_encoded_words.append(X_words_to_indices.get(val))\n",
    "        X_encoded += [X_encoded_words]\n",
    "    return X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentences, word indices [359699, 143138, 107474, 318005, 271940, 361488, 195554, 126463, 391264, 161837, 48420, 363369, 109493, 363369, 332754, 85853, 218261, 375629, 324031, 123767, 389005, 231734, 112304, 126757, 92049, 72526, 366525, 363369, 330316, 936]\n",
      "\n",
      "First sentences, LEN indices [0, 4, 8, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "X_only_indices = encode_to_indices(X, X_words_to_indices)\n",
    "y_only_indices = encode_to_indices(y, y_len_to_indices)\n",
    "print('First sentences, word indices', X_only_indices[4])\n",
    "print(\"\")\n",
    "print('First sentences, LEN indices', y_only_indices[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_embeddings(embeddings_dict, key_word, n):\n",
    "    \n",
    "    d = {}\n",
    "    key_word_embedding = embeddings_dict[key_word]\n",
    "    \n",
    "    for word, embedding in embeddings_dict.items():\n",
    "        d_val = cosine_similarity([key_word_embedding], [embedding])[0][0]\n",
    "        d[word] = d_val\n",
    "    \n",
    "    d_sorted = sorted(d.items(), key=itemgetter(1), reverse=True)[1:n+1] # Do not return table\n",
    "        \n",
    "    return d_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table = most_similar_embeddings(embeddings_dict, 'table', 5)\n",
    "#france = most_similar_embeddings(embeddings_dict, 'france', 5)\n",
    "#sweden = most_similar_embeddings(embeddings_dict, 'sweden', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(table)\n",
    "#print(france)\n",
    "#print(sweden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_glove_matrix(X_vocabulary, embeddings_dict):\n",
    "    for word in X_vocabulary:\n",
    "        if word in embeddings_dict:\n",
    "            i = X_words_to_indices[word]\n",
    "            embedding = embeddings_dict[word]\n",
    "            word_embedding_matrix[i] = embedding\n",
    "    return word_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (402598, 100)\n"
     ]
    }
   ],
   "source": [
    "word_embedding_matrix = np.random.random((len(X_vocabulary)+2, 100))\n",
    "word_embedding_matrix = fill_glove_matrix(X_vocabulary, embeddings_dict)\n",
    "print('Shape of embedding matrix:', word_embedding_matrix.shape)\n",
    "#print('Embedding of the padding symbol, idx 0, random numbers', word_embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the longest sequence in either train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length in train:  113\n",
      "Maximum sentence length in val:  109\n",
      "Maximum sentence length in test:  124\n",
      "Maximum sentence length total:  124\n"
     ]
    }
   ],
   "source": [
    "max_seq_len_train = max(len(s) for s in X)\n",
    "print(\"Maximum sentence length in train: \", max_seq_len_train)\n",
    "\n",
    "X_val, _ = extract_features(val_dict)\n",
    "max_seq_len_val = max(len(s) for s in X_val)\n",
    "print(\"Maximum sentence length in val: \", max_seq_len_val)\n",
    "\n",
    "X_test, _ = extract_features(test_dict)\n",
    "max_seq_len_test = max(len(s) for s in X_test)\n",
    "print(\"Maximum sentence length in test: \", max_seq_len_test)\n",
    "\n",
    "max_seq_len = max(max_seq_len_train, max_seq_len_val, max_seq_len_test)\n",
    "\n",
    "print(\"Maximum sentence length total: \", max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0 142144 307144 161837  91322 363369\n",
      "  83767  85853 218261    936]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 4 0 3 0 0 0 3 0 0]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(X_only_indices, maxlen=max_seq_len)\n",
    "y_train = pad_sequences(y_only_indices, maxlen=max_seq_len)\n",
    "\n",
    "print(X_train[1])\n",
    "print(y_train[1])\n",
    "\n",
    "# The number of classes and 0 (padding symbol)\n",
    "y_train = to_categorical(y_train, num_classes=nbr_of_classes + 1)\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0 113352    679 221876 354361 275585  63472 364506\n",
      "  49151 192164 381012    936]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 4 0 0 0 0 0 0 0 0]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X_val, y_val = extract_features(val_dict)\n",
    "\n",
    "# We create the parallel sequences of indexes\n",
    "X_only_indices_val = encode_to_indices(X_val, X_words_to_indices, num_words=total_word_count)\n",
    "y_only_indices_val = encode_to_indices(y_val, y_len_to_indices)\n",
    "\n",
    "X_val = pad_sequences(X_only_indices_val, maxlen=max_seq_len)\n",
    "y_val = pad_sequences(y_only_indices_val, maxlen=max_seq_len)\n",
    "\n",
    "print(X_val[1])\n",
    "print(y_val[1])\n",
    "\n",
    "y_val = to_categorical(y_val, num_classes=nbr_of_classes + 1)\n",
    "print(y_val[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIMPLE RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simpleRNN():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_word_count+2,\n",
    "                               100,\n",
    "                               mask_zero=True,\n",
    "                               input_length=max_seq_len))\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    model.layers[0].trainable = True\n",
    "    model.add(SimpleRNN(32, return_sequences=True))\n",
    "    model.add(Dense(nbr_of_classes + 1, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_word_count+2,\n",
    "                               100,\n",
    "                               mask_zero=True,\n",
    "                               input_length=max_seq_len))\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    model.layers[0].trainable = True\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dense(nbr_of_classes + 1, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM BIDIRECTIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM_BIDIRECTIONAL():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_word_count+2,\n",
    "                               100,\n",
    "                               mask_zero=True,\n",
    "                               input_length=max_seq_len))\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    model.layers[0].trainable = True\n",
    "    model.add(Bidirectional(layers.SimpleRNN(nbr_of_classes + 1, return_sequences=True)))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dense(nbr_of_classes + 1, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_LSTM_BIDIRECTIONAL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 124, 100)          40259800  \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 124, 24)           2712      \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 124, 128)          78336     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 124, 12)           1548      \n",
      "=================================================================\n",
      "Total params: 40,342,396\n",
      "Trainable params: 40,342,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/1\n",
      "14987/14987 [==============================] - 106s 7ms/step - loss: 0.7768 - acc: 0.8242 - val_loss: 0.5338 - val_acc: 0.8478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6388def080>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0 338645    679 197601 162138 229068 390519    517 100681\n",
      " 190292 350950 120819    936]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 5 0 0 0 0]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = extract_features(test_dict)\n",
    "\n",
    "# We create the parallel sequences of indexes\n",
    "X_only_indices_test = encode_to_indices(X_test, X_words_to_indices, num_words=total_word_count)\n",
    "y_only_indices_test = encode_to_indices(y_test, y_len_to_indices)\n",
    "\n",
    "X_test_padded = pad_sequences(X_only_indices_test, maxlen=max_seq_len)\n",
    "y_test_padded = pad_sequences(y_only_indices_test, maxlen=max_seq_len)\n",
    "\n",
    "print(X_test_padded[1])\n",
    "print(y_test_padded[1])\n",
    "\n",
    "# The number of LEN classes and 0 (padding symbol)\n",
    "y_test_vectorized = to_categorical(y_test_padded, num_classes=nbr_of_classes + 1)\n",
    "print(y_test_vectorized[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'UNKNOWN_WORD',\n",
       " 2: 'B-LOC',\n",
       " 3: 'B-MISC',\n",
       " 4: 'B-ORG',\n",
       " 5: 'B-PER',\n",
       " 6: 'I-LOC',\n",
       " 7: 'I-MISC',\n",
       " 8: 'I-ORG',\n",
       " 9: 'I-PER'}"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_indices_to_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684 3684 3684 3684\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test), len(y_test), len(X_only_indices_test), len(y_only_indices_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 5s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test_padded, y_test_vectorized, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5609088026868921\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8352069379711254\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connleval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = model.predict(X_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.7175146e-02, 8.0977969e-02, 8.4987573e-02, ..., 8.3782949e-02,\n",
       "        8.1285521e-02, 8.1265487e-02],\n",
       "       [8.7175146e-02, 8.0977969e-02, 8.4987573e-02, ..., 8.3782949e-02,\n",
       "        8.1285521e-02, 8.1265487e-02],\n",
       "       [8.7175146e-02, 8.0977969e-02, 8.4987573e-02, ..., 8.3782949e-02,\n",
       "        8.1285521e-02, 8.1265487e-02],\n",
       "       ...,\n",
       "       [9.3399054e-01, 1.1692542e-04, 2.3872891e-02, ..., 1.2507252e-03,\n",
       "        7.9098267e-05, 1.0645347e-04],\n",
       "       [9.6727854e-01, 6.9592221e-05, 1.1995676e-02, ..., 8.4547827e-04,\n",
       "        4.0951207e-05, 6.5927372e-05],\n",
       "       [9.8896211e-01, 1.6966827e-05, 4.4216886e-03, ..., 1.5991685e-04,\n",
       "        9.9133158e-06, 1.3971513e-05]], dtype=float32)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_probs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test ['soccer', '-', 'japan', 'get', 'lucky', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.']\n",
      "X_test_padded [     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0 338645    679 197601 162138 229068 390519    517 100681\n",
      " 190292 350950 120819    936]\n",
      "Y_test ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O']\n",
      "Y_test_padded [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 5 0 0 0 0]\n",
      "predictions [[8.7175146e-02 8.0977969e-02 8.4987573e-02 ... 8.3782949e-02\n",
      "  8.1285521e-02 8.1265487e-02]\n",
      " [8.7175146e-02 8.0977969e-02 8.4987573e-02 ... 8.3782949e-02\n",
      "  8.1285521e-02 8.1265487e-02]\n",
      " [8.7175146e-02 8.0977969e-02 8.4987573e-02 ... 8.3782949e-02\n",
      "  8.1285521e-02 8.1265487e-02]\n",
      " ...\n",
      " [9.3399054e-01 1.1692542e-04 2.3872891e-02 ... 1.2507252e-03\n",
      "  7.9098267e-05 1.0645347e-04]\n",
      " [9.6727854e-01 6.9592221e-05 1.1995676e-02 ... 8.4547827e-04\n",
      "  4.0951207e-05 6.5927372e-05]\n",
      " [9.8896211e-01 1.6966827e-05 4.4216886e-03 ... 1.5991685e-04\n",
      "  9.9133158e-06 1.3971513e-05]]\n"
     ]
    }
   ],
   "source": [
    "print('X_test', X_test[1])\n",
    "print('X_test_padded', X_test_padded[1])\n",
    "print('Y_test', y_test[1])\n",
    "print('Y_test_padded', y_test_padded[1])\n",
    "print('predictions', y_pred_probs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7705599  0.00751109 0.04419868 0.02887081 0.03264279 0.02014346\n",
      "  0.01727237 0.01992202 0.02555126 0.01793962 0.00776042 0.00762758]]\n"
     ]
    }
   ],
   "source": [
    "# Remove padding\n",
    "y_pred_probs_no_padd = []\n",
    "for sent_nbr, sent_len_predictions in enumerate(corpus_pos_predictions):\n",
    "    y_pred_probs_no_padd += [sent_len_predictions[-len(X_test[sent_nbr]):]]\n",
    "print(y_pred_probs_no_padd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], [None, None]]\n",
      "\n",
      "[['O'], ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O'], ['B-PER', 'I-PER']]\n"
     ]
    }
   ],
   "source": [
    "# Extract prediction with highest probability and convert indices to symbols\n",
    "y_pred = []\n",
    "for sentence in y_pred_probs_no_padd:\n",
    "    len_idx = list(map(np.argmax, sentence))\n",
    "    len_cat = list(map(y_indices_to_len.get, len_idx))\n",
    "    y_pred += [len_cat]\n",
    "\n",
    "print(y_pred[:3])\n",
    "print(\"\")\n",
    "print(y_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3684, 3684)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46666\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for arr in y_test:\n",
    "    for x in arr:\n",
    "        c += 1\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46666\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for arr in y_pred:\n",
    "    for x in arr:\n",
    "        c += 1\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39616\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i, arr in enumerate(y_test):\n",
    "    for j, word in enumerate(arr):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            if y_pred[i][j] == y_test[i][j]:\n",
    "                c += 1\n",
    "        except:\n",
    "            print(i, j)\n",
    "\n",
    "    \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 124)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred[213]), len(y_test[213])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1143\n"
     ]
    }
   ],
   "source": [
    "total, correct, total_ukn, correct_ukn = 0, 0, 0, 0\n",
    "for id_s, sentence in enumerate(X_test):\n",
    "    for id_w, word in enumerate(sentence):\n",
    "        \n",
    "        if word not in X_words_to_indices:\n",
    "            total += 1\n",
    "            \n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 41620, correct 39616, accuracy 0.951850\n",
      "total unknown 1143, correct 861, accuracy 0.753281\n"
     ]
    }
   ],
   "source": [
    "total, correct, total_ukn, correct_ukn = 0, 0, 0, 0\n",
    "missing = 0\n",
    "\n",
    "for id_s, sentence in enumerate(X_test):\n",
    "    for id_w, word in enumerate(sentence):\n",
    "        \n",
    "        try:\n",
    "            if y_pred[id_s][id_w] == y_test[id_s][id_w]:\n",
    "                correct += 1\n",
    "        except:\n",
    "            print(id_s, id_w)\n",
    "            missing += 1\n",
    "        # The word is not in the dictionary\n",
    "        if word not in X_words_to_indices:\n",
    "            total_ukn += 1\n",
    "            if pos_pred[id_s][id_w] == y_test[id_s][id_w]:\n",
    "                correct_ukn += 1\n",
    "\n",
    "total += correct\n",
    "total += total_ukn\n",
    "total += correct_ukn\n",
    "print('total %d, correct %d, accuracy %f' % (total, correct, correct / total))\n",
    "if total_ukn != 0:\n",
    "    print('total unknown %d, correct %d, accuracy %f' % (total_ukn, correct_ukn, correct_ukn / total_ukn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"F1_SCORE.txt\", \"w\")\n",
    "\n",
    "x = 0\n",
    "\n",
    "for id_s, sentence in enumerate(y_test):\n",
    "    \n",
    "    #if x == 1000:\n",
    "    #    break\n",
    "        \n",
    "    for id_w, word in enumerate(sentence):\n",
    "        x += 1\n",
    "        \n",
    "        word = test_dict[id_s][id_w]['form']\n",
    "        ppos = test_dict[id_s][id_w]['ppos']\n",
    "        pchunck = test_dict[id_s][id_w]['pchunk']\n",
    "        y_true = y_test[id_s][id_w]\n",
    "        pred = y_pred[id_s][id_w]\n",
    "\n",
    "        f.write(str(word) + \" \" + str(ppos) + \" \" + str(pchunck) + \" \" + str(y_true).upper() + \" \" + str(pred).upper() + \"\\n\")\n",
    "\n",
    "        if word == \"-DOCSTART-\" or word == \".\":\n",
    "            f.write(\"\\n\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(id_s, id_w)\n",
    "        \n",
    "f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
