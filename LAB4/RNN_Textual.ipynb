{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(13)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(13)\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, MaxPooling3D, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2             \n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'datasets\\test.txt', sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '-DOCSTART- -X- -X- O'}]\n",
      "{'id': '-DOCSTART- -X- -X- O'}\n",
      "<class '__main__.Token'>\n",
      "[{'id': 'EU NNP B-NP B-ORG'}, {'id': 'rejects VBZ B-VP O'}, {'id': 'German JJ B-NP B-MISC'}, {'id': 'call NN I-NP O'}, {'id': 'to TO B-VP O'}, {'id': 'boycott VB I-VP O'}, {'id': 'British JJ B-NP B-MISC'}, {'id': 'lamb NN I-NP O'}, {'id': '. . O O'}]\n",
      "La\n",
      "True\n",
      "{'cpos', 'pos', 'feats', 'form', 'id', 'lemma'}\n",
      "{'pos', 'id', 'lemma', 'cpos', 'feats', 'form'}\n",
      "dict_keys(['id', 'form', 'lemma', 'cpos', 'pos', 'feats'])\n",
      "['el', 'La', '1', 'num=s|gen=f', 'da', 'd']\n",
      "['el', 'La', '1', 'num=s|gen=f', 'da', 'd']\n",
      "['el', 'La', '1', 'num=s|gen=f', 'da', 'd']\n",
      "Token value: dict_values(['1', 'La', 'el', 'd', 'da', 'num=s|gen=f'])\n",
      "['d', 'el', 'La', '1', 'num=s|gen=f', 'da']\n"
     ]
    }
   ],
   "source": [
    "# %load conll_dictorizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'form': '-DOCSTART-', 'ppos': '-X-', 'pchunk': '-X-', 'ner': 'O'}]\n",
      "[{'form': 'EU', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-ORG'}, {'form': 'rejects', 'ppos': 'VBZ', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'German', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'call', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'boycott', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'British', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}]\n"
     ]
    }
   ],
   "source": [
    "# %load datasets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dict is a list of lists of dictionaries\n",
    "def extract_WORDS_NER_TAGS(train_dict):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for list_dict in train_dict: # list of dicts\n",
    "        for d in list_dict:\n",
    "            word = d['form'].lower()\n",
    "            ner = d['ner'].lower()\n",
    "            if word != \"-docstart-\":\n",
    "                X.append(word)\n",
    "                Y.append(ner)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices_words(X, glove):\n",
    "    d_indices = {}\n",
    "    # 0 = padding,  1 = unknown\n",
    "    c = 2\n",
    "    \n",
    "    # Add all glove words\n",
    "    for word in glove:\n",
    "        if word not in d_indices:\n",
    "            d_indices[word] = c\n",
    "            c += 1\n",
    "            \n",
    "    # Add all words from training set       \n",
    "    for word in X:\n",
    "        if word not in d_indices:\n",
    "            d_indices[word] = c\n",
    "            c += 1\n",
    "        \n",
    "    return d_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices_NER(Y):\n",
    "    d_indices = {}\n",
    "    c = 0\n",
    "    \n",
    "    # Add all NER tags from training set       \n",
    "    for tag in Y:\n",
    "        if tag not in d_indices:\n",
    "            d_indices[tag] = c\n",
    "            c += 1\n",
    "        \n",
    "    return d_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_indices(d_indices):\n",
    "    return {v: k for k, v in d_indices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_GloVe(file):\n",
    "    embeddings_dict = {}\n",
    "    glove = open(file, encoding='utf-8')\n",
    "    \n",
    "    for line in glove:\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding_vec_word = np.array(line[1:], dtype='float32')\n",
    "        embeddings_dict[word] = embedding_vec_word\n",
    "        \n",
    "    glove.close()\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract WORDS and NER Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = extract_WORDS_NER_TAGS(train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load GloVe vector embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = load_GloVe('glove.6B/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create indices and inverted indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_indices = create_indices_words(X, embeddings_dict)\n",
    "Y_indices = create_indices_NER(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_inv_indicies = create_inverted_indices(X_indices)\n",
    "Y_inv_indicies = create_inverted_indices(Y_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate most similar word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_embeddings(embeddings_dict, key_word, n):\n",
    "    \n",
    "    cs_dict = {}\n",
    "    key_word_embedding = embeddings_dict[key_word]\n",
    "    \n",
    "    for word, embedding in embeddings_dict.items():\n",
    "        cs = cosine_similarity([key_word_embedding], [embedding])[0][0]\n",
    "        cs_dict[word] = cs\n",
    "    \n",
    "    cs_sorted = sorted(cs_dict.items(), key=itemgetter(1), reverse=True)[1:n+1] # Do not return table\n",
    "        \n",
    "    return cs_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_most_similar = most_similar_embeddings(embeddings_dict, 'table', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tables', 0.8021162),\n",
       " ('place', 0.6582378),\n",
       " ('bottom', 0.65597194),\n",
       " ('room', 0.6543691),\n",
       " ('side', 0.6433667)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_most_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create GloVe word embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_GloVe_matrix(X_indices, embeddings_dict):\n",
    "    c = 0\n",
    "    for word in X_indices:\n",
    "        embedding = embeddings_dict.get(word)\n",
    "        if embedding is not None:\n",
    "            word_embedding_matrix[c] = embedding\n",
    "        else:\n",
    "            word_embedding_matrix[c] = np.random.rand(1, 100)\n",
    "        c += 1\n",
    "    return word_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = np.random.rand(len(X_indices), 100)\n",
    "word_embedding_matrix = fill_GloVe_matrix(X_indices, embeddings_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create sequenced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(X, Y, X_indices, Y_indices):\n",
    "    # Find sequences. Each sequence is seperated by '.'\n",
    "    X_all_seq = []\n",
    "    X_seq = []\n",
    "    Y_all_seq = []\n",
    "    Y_seq = []\n",
    "    \n",
    "    for i, w in enumerate(X):\n",
    "        X_seq.append(X_indices.get(w))\n",
    "        Y_seq.append(Y_indices.get(Y[i]))\n",
    "\n",
    "        if w == '.':\n",
    "            X_all_seq.append(X_seq)\n",
    "            X_seq = []\n",
    "            Y_all_seq.append(Y_seq)\n",
    "            Y_seq = []\n",
    "\n",
    "    max_len = max(len(l) for l in X_all_seq)\n",
    "        \n",
    "    X_all_seq = pad_sequences(X_all_seq, max_len)\n",
    "    Y_all_seq = pad_sequences(Y_all_seq, max_len)\n",
    "    \n",
    "    return X_all_seq, Y_all_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq, Y_seq = build_sequences(X, Y, X_indices, Y_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
