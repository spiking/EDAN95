{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(13)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(13)\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, MaxPooling3D, Dropout, Embedding, SimpleRNN, LSTM, GRU\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tempfile import TemporaryFile\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2             \n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load conll_dictorizer.py\n",
    "\"\"\"\n",
    "CoNLL 2009 file readers and writers for the parts of speech.\n",
    "Version with a class modeled as a vectorizer\n",
    "\"\"\"\n",
    "__author__ = \"Pierre Nugues\"\n",
    "\n",
    "import regex as re\n",
    "\n",
    "\n",
    "def save(file, corpus_dict, column_names):\n",
    "    \"\"\"\n",
    "    Saves the corpus in a file\n",
    "    :param file:\n",
    "    :param corpus_dict:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(file, 'w') as f_out:\n",
    "        for sentence in corpus_dict:\n",
    "            sentence_lst = []\n",
    "            for row in sentence:\n",
    "                items = map(lambda x: row.get(x, '_'), column_names)\n",
    "                sentence_lst += '\\t'.join(items) + '\\n'\n",
    "            sentence_lst += '\\n'\n",
    "            f_out.write(''.join(sentence_lst))\n",
    "\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    BASE = os.getcwd()\n",
    "    train_file = os.path.join(BASE, 'datasets/train.txt')\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'cpos', 'pos', 'feats']\n",
    "    train = open(train_file).read().strip()\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "    train_dict = conll_dict.transform(train)\n",
    "\n",
    "    print(train_dict[0])\n",
    "    print(train_dict[0][0])\n",
    "    print(type(train_dict[0][0]))\n",
    "    #print(train_dict[0][0]['form'])\n",
    "    print(train_dict[1])\n",
    "    tok = Token({'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'})\n",
    "    print(tok['form'])\n",
    "    print('form' in tok)\n",
    "\n",
    "    save('out', train_dict, column_names)\n",
    "\n",
    "    tok_dict = {'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'}\n",
    "    tok_dict2 = {'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'}\n",
    "\n",
    "    tok_set = set(tok_dict)\n",
    "    print(tok_set)\n",
    "\n",
    "    tok_set = tok_set.union(tok_dict2)\n",
    "    #print(tok_set)\n",
    "\n",
    "    #print(tok.keys())\n",
    "\n",
    "    # exit()\n",
    "    word_set = set()\n",
    "    word_set = set(tok_dict.values())\n",
    "    #print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    word_set = set(tok.values())\n",
    "    #print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    word_set.update(tok.values())\n",
    "    #print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    #print(\"Token value:\", tok.values())\n",
    "    word_set = word_set.union(set(tok.values()))\n",
    "    #print(list(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load datasets.py\n",
    "from conll_dictorizer import CoNLLDictorizer, Token\n",
    "import os\n",
    "\n",
    "def load_conll2009_pos():\n",
    "    train_file = 'datasets\\train.txt'\n",
    "    dev_file = 'datasets\\valid.txt'\n",
    "    test_file = 'datasets\\test.txt'\n",
    "    test2_file = 'simple_pos_test.txt'\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'plemma', 'pos', 'ppos']\n",
    "\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "def load_conll2003_en():\n",
    "    BASE_DIR = os.getcwd()\n",
    "    train_file = BASE_DIR + '/datasets/train.txt'\n",
    "    dev_file = BASE_DIR + '/datasets/valid.txt'\n",
    "    test_file = BASE_DIR + '/datasets/test.txt'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "    train_dict = conll_dict.transform(train_sentences)\n",
    "    val_dict = conll_dict.transform(dev_sentences)\n",
    "    test_dict = conll_dict.transform(test_sentences)\n",
    "    print(train_dict[0])\n",
    "    print(train_dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(file):\n",
    "    embeddings_dict = {}\n",
    "    glove = open(file, encoding='utf-8')\n",
    "    \n",
    "    for line in glove:\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding_vec_word = np.array(line[1:], dtype='float32')\n",
    "        embeddings_dict[word] = embedding_vec_word\n",
    "        \n",
    "    glove.close()\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_name):\n",
    "    with open('files/' + file_name + '.pkl', 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(file_name, file):\n",
    "    with open('files/' + file_name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file('X_train', X_train)\n",
    "save_file('y_train', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_file('X')\n",
    "y = load_file('y')\n",
    "X_indices_to_words = load_file('X_indices_to_words')\n",
    "y_indices_to_len = load_file('y_indices_to_len')\n",
    "X_words_to_indices = load_file('X_words_to_indices')\n",
    "y_len_to_indices = load_file('y_len_to_indices')\n",
    "X_vocabulary = load_file('X_vocabulary')\n",
    "y_vocabulary = load_file('X_vocabulary')\n",
    "X_only_indices = load_file('X_only_indices')\n",
    "y_only_indices = load_file('y_only_indices')\n",
    "word_embedding_matrix = load_file('word_embedding_matrix')\n",
    "embeddings_dict = load_file('embeddings_dict')\n",
    "cs_most_similar = load_file('cs_most_similar')\n",
    "X_train = load_file('X_train')\n",
    "y_train = load_file('y_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_dict is a list of lists of dictionaries\n",
    "def extract_features(train_dict):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for sentence in train_dict:\n",
    "        X_sentence = []\n",
    "        y_sentence = []\n",
    "        for word in sentence:\n",
    "            w = word['form'].lower()\n",
    "            n = word['ner'].lower()\n",
    "            X_sentence.append(w)\n",
    "            y_sentence.append(n)\n",
    "    \n",
    "        X.append(X_sentence)\n",
    "        y.append(y_sentence)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract words and ner tags - X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = extract_features(train_dict)\n",
    "print('Sentence words: ', X[1])\n",
    "print('Sentence NER: ', y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(X, WORDS=True):\n",
    "    X_vocabulary = set()\n",
    "    for sentence in X:\n",
    "        for word in sentence:\n",
    "            X_vocabulary.add(word)\n",
    "    \n",
    "    if WORDS:\n",
    "        X_vocabulary.add(\"UNKNOWN WORD\")\n",
    "    \n",
    "    return list(X_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vocabulary = create_vocabulary(X)\n",
    "print(\"Vocabulary size WORDS: \", len(X_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vocabulary = create_vocabulary(y, WORDS=True)\n",
    "print(\"Vocabulary size NER: \", len(y_vocabulary))\n",
    "nbr_of_classes = len(y_vocabulary) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add words from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in embeddings_dict.keys():\n",
    "    X_vocabulary.append(word)\n",
    "\n",
    "X_vocabulary = set(X_vocabulary)\n",
    "total_word_count = len(X_vocabulary)\n",
    "print('Words in the vocabulary total, X_vocabulary:', total_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create indices and inverted indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices(X):\n",
    "    return dict(enumerate(X), start=2) # 0 is padding, 1 is unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_indices_to_words = create_indices(X_vocabulary) \n",
    "y_indices_to_len = create_indices(y_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_indices(X):\n",
    "    return {v: k for k, v in X.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words_to_indices = create_inverted_indices(X_indices_to_words)\n",
    "y_len_to_indices = create_inverted_indices(y_indices_to_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Word index:', list(X_words_to_indices.items())[:3])\n",
    "print('LEN index:', list(y_len_to_indices.items())[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode lists - Convert to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_to_indices(X, X_words_to_indices, num_words=None):\n",
    "    X_encoded = []\n",
    "    for x in X:\n",
    "        X_encoded_words = []\n",
    "        if num_words:\n",
    "            # We map the unknown words to the second first index of the matrix, for the test set\n",
    "            X_encoded_words = list(map(lambda x: X_words_to_indices.get(x, 1), x))\n",
    "        else:\n",
    "            for val in x:\n",
    "                X_encoded_words.append(X_words_to_indices.get(val))\n",
    "        X_encoded += [X_encoded_words]\n",
    "    return X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the parallel sequences of indexes\n",
    "X_only_indices = encode_to_indices(X, X_words_to_indices)\n",
    "y_only_indices = encode_to_indices(y, y_len_to_indices)\n",
    "print('First sentences, word indices', X_only_indices[4])\n",
    "print(\"\")\n",
    "print('First sentences, POS indices', y_only_indices[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_embeddings(embeddings_dict, key_word, n):\n",
    "    \n",
    "    d = {}\n",
    "    key_word_embedding = embeddings_dict[key_word]\n",
    "    \n",
    "    for word, embedding in embeddings_dict.items():\n",
    "        d_val = cosine_similarity([key_word_embedding], [embedding])[0][0]\n",
    "        d[word] = d_val\n",
    "    \n",
    "    d_sorted = sorted(d.items(), key=itemgetter(1), reverse=True)[1:n+1] # Do not return table\n",
    "        \n",
    "    return d_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cs_most_similar = most_similar_embeddings(embeddings_dict, 'table', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most similar words: \")\n",
    "cs_most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_glove_matrix(X_vocabulary, embeddings_dict):\n",
    "    for word in X_vocabulary:\n",
    "        if word in embeddings_dict:\n",
    "            i = X_words_to_indices[word]\n",
    "            embedding = embeddings_dict[word]\n",
    "            word_embedding_matrix[i] = embedding\n",
    "    return word_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = np.random.random((len(X_vocabulary)+2, 100))\n",
    "word_embedding_matrix = fill_glove_matrix(X_vocabulary, embeddings_dict)\n",
    "print('Shape of embedding matrix:', word_embedding_matrix.shape)\n",
    "print('Embedding of the padding symbol, idx 0, random numbers', word_embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = max(len(s) for s in X)\n",
    "print(\"Maximum sentence length: \", max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_only_indices, maxlen=max_seq_len, padding=\"post\")\n",
    "y_train = pad_sequences(y_only_indices, maxlen=max_seq_len, padding=\"post\")\n",
    "\n",
    "print(X_train[1])\n",
    "print(y_train[1])\n",
    "\n",
    "# The number of POS classes and 0 (padding symbol)\n",
    "y_train = to_categorical(y_train, num_classes=nbr_of_classes + 1)\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = extract_features(val_dict)\n",
    "\n",
    "print(len(val_dict))\n",
    "\n",
    "# We create the parallel sequences of indexes\n",
    "X_only_indices_val = encode_to_indices(X_val, X_words_to_indices)\n",
    "y_only_indices_val = encode_to_indices(y_val, y_len_to_indices)\n",
    "\n",
    "X_val = pad_sequences(X_only_indices, maxlen=max_seq_len, padding=\"post\")\n",
    "y_val = pad_sequences(y_only_indices, maxlen=max_seq_len, padding=\"post\")\n",
    "\n",
    "print(X_val[1])\n",
    "print(y_val[1])\n",
    "\n",
    "# The number of POS classes and 0 (padding symbol)\n",
    "y_val = to_categorical(y_val, num_classes=nbr_of_classes + 1)\n",
    "print(y_val[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simpleRNN():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(X_vocabulary)+2,\n",
    "                               100,\n",
    "                               mask_zero=True,\n",
    "                               input_length=max_seq_len))\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    # The default is True\n",
    "    model.layers[0].trainable = True\n",
    "    #model.add(layers.Bidirectional(layers.SimpleRNN(NB_CLASSES + 1, return_sequences=True)))\n",
    "    #model.add(layers.Bidirectional(layers.LSTM(NB_CLASSES + 1, return_sequences=True)))\n",
    "    model.add(SimpleRNN(128, return_sequences=True))\n",
    "    #model.add(layers.Bidirectional(layers.LSTM(LSTM_UNITS, return_sequences=True)))\n",
    "    model.add(Dense(nbr_of_classes + 1, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(X_vocabulary)+2,\n",
    "                               100,\n",
    "                               mask_zero=True,\n",
    "                               input_length=max_seq_len))\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    # The default is True\n",
    "    model.layers[0].trainable = True\n",
    "    #model.add(layers.Bidirectional(layers.SimpleRNN(NB_CLASSES + 1, return_sequences=True)))\n",
    "    #model.add(layers.Bidirectional(layers.LSTM(NB_CLASSES + 1, return_sequences=True)))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dense(nbr_of_classes + 1, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM BIDIRECTIONAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_simpleRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = extract_features(test_dict)\n",
    "\n",
    "# We create the parallel sequences of indexes\n",
    "X_only_indices_test = encode_to_indices(X_test, X_words_to_indices, num_words=total_word_count)\n",
    "y_only_indices_test = encode_to_indices(y_test, y_len_to_indices)\n",
    "\n",
    "X_test_padded = pad_sequences(X_only_indices_test, maxlen=max_seq_len, padding=\"post\")\n",
    "y_test_padded = pad_sequences(y_only_indices_test, maxlen=max_seq_len, padding=\"post\")\n",
    "\n",
    "print(X_test_padded[1])\n",
    "print(y_test_padded[1])\n",
    "\n",
    "# The number of POS classes and 0 (padding symbol)\n",
    "y_test_vectorized = to_categorical(y_test_padded, num_classes=nbr_of_classes + 1)\n",
    "print(y_test_vectorized[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test_padded, y_test_vectorized, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connleval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_pos_predictions = model.predict_classes(X_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
