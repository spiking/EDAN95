{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(13)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(13)\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, MaxPooling3D, Dropout, Embedding, Bidirectional, SimpleRNN, LSTM, GRU\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json \n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tempfile import TemporaryFile\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '-DOCSTART- -X- -X- O'}]\n",
      "{'id': '-DOCSTART- -X- -X- O'}\n",
      "<class '__main__.Token'>\n",
      "[{'id': 'EU NNP B-NP B-ORG'}, {'id': 'rejects VBZ B-VP O'}, {'id': 'German JJ B-NP B-MISC'}, {'id': 'call NN I-NP O'}, {'id': 'to TO B-VP O'}, {'id': 'boycott VB I-VP O'}, {'id': 'British JJ B-NP B-MISC'}, {'id': 'lamb NN I-NP O'}, {'id': '. . O O'}]\n",
      "La\n",
      "True\n",
      "{'lemma', 'form', 'pos', 'id', 'cpos', 'feats'}\n"
     ]
    }
   ],
   "source": [
    "# %load conll_dictorizer.py\n",
    "\"\"\"\n",
    "CoNLL 2009 file readers and writers for the parts of speech.\n",
    "Version with a class modeled as a vectorizer\n",
    "\"\"\"\n",
    "__author__ = \"Pierre Nugues\"\n",
    "\n",
    "import regex as re\n",
    "\n",
    "\n",
    "def save(file, corpus_dict, column_names):\n",
    "    \"\"\"\n",
    "    Saves the corpus in a file\n",
    "    :param file:\n",
    "    :param corpus_dict:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(file, 'w') as f_out:\n",
    "        for sentence in corpus_dict:\n",
    "            sentence_lst = []\n",
    "            for row in sentence:\n",
    "                items = map(lambda x: row.get(x, '_'), column_names)\n",
    "                sentence_lst += '\\t'.join(items) + '\\n'\n",
    "            sentence_lst += '\\n'\n",
    "            f_out.write(''.join(sentence_lst))\n",
    "\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    BASE = os.getcwd()\n",
    "    train_file = os.path.join(BASE, 'datasets/train.txt')\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'cpos', 'pos', 'feats']\n",
    "    train = open(train_file).read().strip()\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "    train_dict = conll_dict.transform(train)\n",
    "\n",
    "    print(train_dict[0])\n",
    "    print(train_dict[0][0])\n",
    "    print(type(train_dict[0][0]))\n",
    "    #print(train_dict[0][0]['form'])\n",
    "    print(train_dict[1])\n",
    "    tok = Token({'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'})\n",
    "    print(tok['form'])\n",
    "    print('form' in tok)\n",
    "\n",
    "    save('out', train_dict, column_names)\n",
    "\n",
    "    tok_dict = {'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'}\n",
    "    tok_dict2 = {'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'}\n",
    "\n",
    "    tok_set = set(tok_dict)\n",
    "    print(tok_set)\n",
    "\n",
    "    tok_set = tok_set.union(tok_dict2)\n",
    "    #print(tok_set)\n",
    "\n",
    "    #print(tok.keys())\n",
    "\n",
    "    # exit()\n",
    "    word_set = set()\n",
    "    word_set = set(tok_dict.values())\n",
    "    #print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    word_set = set(tok.values())\n",
    "    #print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    word_set.update(tok.values())\n",
    "    #print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    #print(\"Token value:\", tok.values())\n",
    "    word_set = word_set.union(set(tok.values()))\n",
    "    #print(list(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'form': '-DOCSTART-', 'ppos': '-X-', 'pchunk': '-X-', 'ner': 'O'}]\n",
      "[{'form': 'EU', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-ORG'}, {'form': 'rejects', 'ppos': 'VBZ', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'German', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'call', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'boycott', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'British', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}]\n"
     ]
    }
   ],
   "source": [
    "# %load datasets.py\n",
    "from conll_dictorizer import CoNLLDictorizer, Token\n",
    "import os\n",
    "\n",
    "def load_conll2009_pos():\n",
    "    train_file = 'datasets\\train.txt'\n",
    "    dev_file = 'datasets\\valid.txt'\n",
    "    test_file = 'datasets\\test.txt'\n",
    "    test2_file = 'simple_pos_test.txt'\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'plemma', 'pos', 'ppos']\n",
    "\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "def load_conll2003_en():\n",
    "    BASE_DIR = os.getcwd()\n",
    "    train_file = BASE_DIR + '/datasets/train.txt'\n",
    "    dev_file = BASE_DIR + '/datasets/valid.txt'\n",
    "    test_file = BASE_DIR + '/datasets/test.txt'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "    train_dict = conll_dict.transform(train_sentences)\n",
    "    val_dict = conll_dict.transform(dev_sentences)\n",
    "    test_dict = conll_dict.transform(test_sentences)\n",
    "    print(train_dict[0])\n",
    "    print(train_dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(file):\n",
    "    embeddings_dict = {}\n",
    "    glove = open(file, encoding='utf-8')\n",
    "    \n",
    "    for line in glove:\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding_vec_word = np.array(line[1:], dtype='float32')\n",
    "        embeddings_dict[word] = embedding_vec_word\n",
    "        \n",
    "    glove.close()\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_name):\n",
    "    with open('files/' + file_name + '.pkl', 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(file_name, file):\n",
    "    with open('files/' + file_name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = load_file('embeddings_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_dict is a list of lists of dictionaries\n",
    "def extract_features(train_dict):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for sentence in train_dict:\n",
    "        X_sentence = []\n",
    "        y_sentence = []\n",
    "        for word in sentence:\n",
    "            w = word['form'].lower()\n",
    "            n = word['ner']\n",
    "            X_sentence.append(w)\n",
    "            y_sentence.append(n)\n",
    "    \n",
    "        X.append(X_sentence)\n",
    "        y.append(y_sentence)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract words and ner tags - X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence words:  ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "Sentence NER:  ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "X, y = extract_features(train_dict)\n",
    "print('Sentence words: ', X[1])\n",
    "print('Sentence NER: ', y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(X, WORDS=True):\n",
    "    X_vocabulary = set()\n",
    "    \n",
    "    if WORDS:\n",
    "        X_vocabulary.add(\"UNKNOWN_WORD\")\n",
    "        \n",
    "    for sentence in X:\n",
    "        for word in sentence:\n",
    "            X_vocabulary.add(word)\n",
    "    \n",
    "    return sorted(list(X_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size WORDS:  21011\n"
     ]
    }
   ],
   "source": [
    "X_vocabulary = create_vocabulary(X, WORDS=True)\n",
    "print(\"Vocabulary size WORDS: \", len(X_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size NER:  9\n"
     ]
    }
   ],
   "source": [
    "y_vocabulary = create_vocabulary(y, WORDS=False)\n",
    "print(\"Vocabulary size NER: \", len(y_vocabulary))\n",
    "nbr_of_classes = len(y_vocabulary) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add words from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in the vocabulary total, X_vocabulary: 402596\n"
     ]
    }
   ],
   "source": [
    "for word in embeddings_dict.keys():\n",
    "    X_vocabulary.append(word)\n",
    "\n",
    "X_vocabulary = sorted(list(set(X_vocabulary)))\n",
    "total_word_count = len(X_vocabulary)\n",
    "print('Words in the vocabulary total, X_vocabulary:', total_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create indices and inverted indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices(X):\n",
    "    return dict(enumerate(X), start=2) # 0 is padding, 1 is unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "X_indices_to_words = {}\n",
    "\n",
    "for w in X_vocabulary:\n",
    "    X_indices_to_words[i] = w\n",
    "    i += 1\n",
    "\n",
    "y_indices_to_len = {}\n",
    "y_indices_to_len[0] = 'O' #PADDING\n",
    "y_indices_to_len[1] = 'UNKNOWN_WORD'\n",
    "i = 2\n",
    "\n",
    "for l in y_vocabulary:\n",
    "    if l != 'O':\n",
    "        y_indices_to_len[i] = l\n",
    "        i += 1\n",
    "        \n",
    "#X_indices_to_len = dict(enumerate(X_vocabulary), start=2)\n",
    "#y_indices_to_len = dict(enumerate(y_vocabulary), start=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_indices(X):\n",
    "    return {v: k for k, v in X.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words_to_indices = create_inverted_indices(X_indices_to_words)\n",
    "y_len_to_indices = create_inverted_indices(y_indices_to_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index: [('!', 2), ('!!', 3), ('!!!', 4)]\n",
      "LEN index: [('O', 0), ('UNKNOWN_WORD', 1), ('B-LOC', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Word index:', list(X_words_to_indices.items())[:3])\n",
    "print('LEN index:', list(y_len_to_indices.items())[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode lists - Convert to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_to_indices(X, X_words_to_indices, num_words=None):\n",
    "    X_encoded = []\n",
    "    for x in X:\n",
    "        X_encoded_words = []\n",
    "        if num_words:\n",
    "            # We map the unknown words to the second first index of the matrix, for the test set\n",
    "            X_encoded_words = list(map(lambda x: X_words_to_indices.get(x,1), x))\n",
    "        else:\n",
    "             X_encoded_words = list(map(X_words_to_indices.get, x))\n",
    "            #for val in x:\n",
    "            #    X_encoded_words.append(X_words_to_indices.get(val))\n",
    "        X_encoded += [X_encoded_words]\n",
    "    return X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentences, word indices [359699, 143138, 107474, 318005, 271940, 361488, 195554, 126463, 391264, 161837, 48420, 363369, 109493, 363369, 332754, 85853, 218261, 375629, 324031, 123767, 389005, 231734, 112304, 126757, 92049, 72526, 366525, 363369, 330316, 936]\n",
      "\n",
      "First sentences, LEN indices [0, 4, 8, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "X_only_indices = encode_to_indices(X, X_words_to_indices)\n",
    "y_only_indices = encode_to_indices(y, y_len_to_indices)\n",
    "print('First sentences, word indices', X_only_indices[4])\n",
    "print(\"\")\n",
    "print('First sentences, LEN indices', y_only_indices[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_embeddings(embeddings_dict, key_word, n):\n",
    "    \n",
    "    d = {}\n",
    "    key_word_embedding = embeddings_dict[key_word]\n",
    "    \n",
    "    for word, embedding in embeddings_dict.items():\n",
    "        d_val = cosine_similarity([key_word_embedding], [embedding])[0][0]\n",
    "        d[word] = d_val\n",
    "    \n",
    "    d_sorted = sorted(d.items(), key=itemgetter(1), reverse=True)[1:n+1] # Do not return table\n",
    "        \n",
    "    return d_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table = most_similar_embeddings(embeddings_dict, 'table', 5)\n",
    "#france = most_similar_embeddings(embeddings_dict, 'france', 5)\n",
    "#sweden = most_similar_embeddings(embeddings_dict, 'sweden', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n[('tables', 0.8021162), ('place', 0.6582378), ('bottom', 0.65597194), ('room', 0.6543691), ('side', 0.6433667)]\\n[('belgium', 0.8076422), ('french', 0.80043775), ('britain', 0.79505277), ('spain', 0.75574636), ('paris', 0.7481586)]\\n[('denmark', 0.86244005), ('norway', 0.807325), ('finland', 0.79064953), ('netherlands', 0.74684644), ('austria', 0.7466836)]\\n\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(table)\n",
    "#print(france)\n",
    "#print(sweden)\n",
    "\n",
    "'''\n",
    "\n",
    "[('tables', 0.8021162), ('place', 0.6582378), ('bottom', 0.65597194), ('room', 0.6543691), ('side', 0.6433667)]\n",
    "[('belgium', 0.8076422), ('french', 0.80043775), ('britain', 0.79505277), ('spain', 0.75574636), ('paris', 0.7481586)]\n",
    "[('denmark', 0.86244005), ('norway', 0.807325), ('finland', 0.79064953), ('netherlands', 0.74684644), ('austria', 0.7466836)]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_glove_matrix(X_vocabulary, embeddings_dict):\n",
    "    for word in X_vocabulary:\n",
    "        if word in embeddings_dict:\n",
    "            i = X_words_to_indices[word]\n",
    "            embedding = embeddings_dict[word]\n",
    "            word_embedding_matrix[i] = embedding\n",
    "    return word_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (402598, 100)\n"
     ]
    }
   ],
   "source": [
    "word_embedding_matrix = np.random.random((len(X_vocabulary)+2, 100))\n",
    "word_embedding_matrix = fill_glove_matrix(X_vocabulary, embeddings_dict)\n",
    "print('Shape of embedding matrix:', word_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the longest sequence in either train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length in train:  113\n",
      "Maximum sentence length in val:  109\n",
      "Maximum sentence length in test:  124\n",
      "Maximum sentence length total:  124\n"
     ]
    }
   ],
   "source": [
    "max_seq_len_train = max(len(s) for s in X)\n",
    "print(\"Maximum sentence length in train: \", max_seq_len_train)\n",
    "\n",
    "X_val, _ = extract_features(val_dict)\n",
    "max_seq_len_val = max(len(s) for s in X_val)\n",
    "print(\"Maximum sentence length in val: \", max_seq_len_val)\n",
    "\n",
    "X_test, _ = extract_features(test_dict)\n",
    "max_seq_len_test = max(len(s) for s in X_test)\n",
    "print(\"Maximum sentence length in test: \", max_seq_len_test)\n",
    "\n",
    "max_seq_len = max(max_seq_len_train, max_seq_len_val, max_seq_len_test)\n",
    "\n",
    "print(\"Maximum sentence length total: \", max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_only_indices, maxlen=max_seq_len)\n",
    "y_train = pad_sequences(y_only_indices, maxlen=max_seq_len)\n",
    "\n",
    "# The number of classes and 0 (padding symbol)\n",
    "y_train = to_categorical(y_train, num_classes=nbr_of_classes + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = extract_features(val_dict)\n",
    "\n",
    "X_only_indices_val = encode_to_indices(X_val, X_words_to_indices, num_words=total_word_count)\n",
    "y_only_indices_val = encode_to_indices(y_val, y_len_to_indices)\n",
    "\n",
    "X_val = pad_sequences(X_only_indices_val, maxlen=max_seq_len)\n",
    "y_val = pad_sequences(y_only_indices_val, maxlen=max_seq_len)\n",
    "\n",
    "y_val = to_categorical(y_val, num_classes=nbr_of_classes + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIMPLE RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simpleRNN():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_word_count+2,\n",
    "                               100,\n",
    "                               mask_zero=True,\n",
    "                               input_length=max_seq_len))\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    model.layers[0].trainable = True\n",
    "    model.add(SimpleRNN(32, return_sequences=True))\n",
    "    model.add(Dense(nbr_of_classes + 1, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_word_count+2,\n",
    "                               100,\n",
    "                               mask_zero=True,\n",
    "                               input_length=max_seq_len))\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    model.layers[0].trainable = True\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dense(nbr_of_classes + 1, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM BIDIRECTIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM_BIDIRECTIONAL():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_word_count+2,\n",
    "                               100,\n",
    "                               mask_zero=True,\n",
    "                               input_length=max_seq_len))\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    model.layers[0].trainable = True\n",
    "    model.add(SimpleRNN(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "    model.add(SimpleRNN(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "    model.add(Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "    model.add(Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "    model.add(Dense(nbr_of_classes + 1, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_simpleRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 124, 100)          40259800  \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 124, 512)          313856    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 124, 512)          1574912   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 124, 12)           6156      \n",
      "=================================================================\n",
      "Total params: 42,154,724\n",
      "Trainable params: 42,154,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=3),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_acc', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/25\n",
      "14987/14987 [==============================] - 12s 805us/step - loss: 0.7853 - acc: 0.8084 - val_loss: 0.5683 - val_acc: 0.8444\n",
      "Epoch 2/25\n",
      "14987/14987 [==============================] - 10s 699us/step - loss: 0.4211 - acc: 0.8872 - val_loss: 0.3441 - val_acc: 0.9132\n",
      "Epoch 3/25\n",
      "14987/14987 [==============================] - 10s 696us/step - loss: 0.2733 - acc: 0.9286 - val_loss: 0.2466 - val_acc: 0.9382\n",
      "Epoch 4/25\n",
      "14987/14987 [==============================] - 10s 697us/step - loss: 0.2028 - acc: 0.9456 - val_loss: 0.2052 - val_acc: 0.9467\n",
      "Epoch 5/25\n",
      "14987/14987 [==============================] - 10s 695us/step - loss: 0.1647 - acc: 0.9540 - val_loss: 0.1810 - val_acc: 0.9515\n",
      "Epoch 6/25\n",
      "14987/14987 [==============================] - 10s 698us/step - loss: 0.1412 - acc: 0.9596 - val_loss: 0.1677 - val_acc: 0.9537\n",
      "Epoch 7/25\n",
      "14987/14987 [==============================] - 11s 712us/step - loss: 0.1240 - acc: 0.9638 - val_loss: 0.1585 - val_acc: 0.9551\n",
      "Epoch 8/25\n",
      "14987/14987 [==============================] - 11s 703us/step - loss: 0.1110 - acc: 0.9672 - val_loss: 0.1537 - val_acc: 0.9575\n",
      "Epoch 9/25\n",
      "14987/14987 [==============================] - 10s 700us/step - loss: 0.1010 - acc: 0.9701 - val_loss: 0.1474 - val_acc: 0.9584\n",
      "Epoch 10/25\n",
      "14987/14987 [==============================] - 10s 700us/step - loss: 0.0923 - acc: 0.9726 - val_loss: 0.1456 - val_acc: 0.9582\n",
      "Epoch 11/25\n",
      "14987/14987 [==============================] - 11s 703us/step - loss: 0.0851 - acc: 0.9747 - val_loss: 0.1456 - val_acc: 0.9607\n",
      "Epoch 12/25\n",
      "14987/14987 [==============================] - 11s 708us/step - loss: 0.0789 - acc: 0.9765 - val_loss: 0.1439 - val_acc: 0.9610\n",
      "Epoch 13/25\n",
      "14987/14987 [==============================] - 10s 697us/step - loss: 0.0735 - acc: 0.9781 - val_loss: 0.1418 - val_acc: 0.9595\n",
      "Epoch 14/25\n",
      "14987/14987 [==============================] - 12s 782us/step - loss: 0.0686 - acc: 0.9793 - val_loss: 0.1421 - val_acc: 0.9592\n",
      "Epoch 15/25\n",
      "14987/14987 [==============================] - 11s 720us/step - loss: 0.0642 - acc: 0.9805 - val_loss: 0.1421 - val_acc: 0.9596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a0cf83d128>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=callbacks, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = extract_features(test_dict)\n",
    "\n",
    "# We create the parallel sequences of indexes\n",
    "X_only_indices_test = encode_to_indices(X_test, X_words_to_indices, num_words=total_word_count)\n",
    "y_only_indices_test = encode_to_indices(y_test, y_len_to_indices)\n",
    "\n",
    "X_test_padded = pad_sequences(X_only_indices_test, maxlen=max_seq_len)\n",
    "y_test_padded = pad_sequences(y_only_indices_test, maxlen=max_seq_len)\n",
    "\n",
    "# The number of LEN classes and 0 (padding symbol)\n",
    "y_test_vectorized = to_categorical(y_test_padded, num_classes=nbr_of_classes + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684 3684 3684 3684\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test), len(y_test), len(X_test_padded), len(y_test_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 5s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test_padded, y_test_vectorized, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.1487194175440636\n",
      "Accuracy:  0.9679915213817882\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSIMPLERNN 32 NODES\\n\\nLoss:  0.19288392033302565\\nAccuracy:  0.942765539360357\\n\\nLSTM_BIDIRECTIONAL + SIMPLERNN\\n\\nLoss:  0.13949491816545542\\nAccuracy:  0.9655883558798303\\n\\n'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "SIMPLERNN 32 NODES\n",
    "\n",
    "Loss:  0.19288392033302565\n",
    "Accuracy:  0.942765539360357\n",
    "\n",
    "LSTM_BIDIRECTIONAL + SIMPLERNN\n",
    "\n",
    "Loss:  0.1487194175440636\n",
    "Accuracy:  0.9679915213817882\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = model.predict(X_test_padded)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('X_test', X_test[1])\n",
    "print('X_test_padded', X_test_padded[1])\n",
    "print('Y_test', y_test[1])\n",
    "print('Y_test_padded', y_test_padded[1])\n",
    "print('predictions', y_pred_probs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove padding and extract pred with highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 1.02456221e-11 3.63981485e-08 1.38617118e-08\n",
      "  1.21875043e-08 2.77253935e-08 6.79346357e-10 1.05724824e-08\n",
      "  5.76844184e-10 7.01748548e-10 4.52988255e-12 4.47230274e-12]]\n"
     ]
    }
   ],
   "source": [
    "# Remove padding\n",
    "y_pred_probs_no_padd = []\n",
    "for sent_nbr, sent_len_predictions in enumerate(y_pred_probs):\n",
    "    y_pred_probs_no_padd += [sent_len_predictions[-len(X_test[sent_nbr]):]]\n",
    "print(y_pred_probs_no_padd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O'], ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O'], ['B-PER', 'I-PER']]\n",
      "\n",
      "[['O'], ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O'], ['B-PER', 'I-PER']]\n"
     ]
    }
   ],
   "source": [
    "# Extract prediction with highest probability and convert indices to symbols\n",
    "y_pred = []\n",
    "for sentence in y_pred_probs_no_padd:\n",
    "    len_idx = list(map(np.argmax, sentence))\n",
    "    len_cat = list(map(y_indices_to_len.get, len_idx))\n",
    "    y_pred += [len_cat]\n",
    "\n",
    "print(y_pred[:3])\n",
    "print(\"\")\n",
    "print(y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  47274\n",
      "Correct:  45155\n",
      "Acc:  0.9551762067944325\n",
      "Unknown:  1143\n",
      "Unknown acc:  0.8538932633420823\n"
     ]
    }
   ],
   "source": [
    "total, correct, unknown, correct_unknown = 0, 0, 0, 0\n",
    "\n",
    "for id_s, sentence in enumerate(X_test):\n",
    "    for id_w, word in enumerate(sentence):\n",
    "        if y_pred[id_s][id_w] == y_test[id_s][id_w]:\n",
    "            correct += 1\n",
    "            \n",
    "        # The word is not in the dictionary\n",
    "        if word not in X_words_to_indices:\n",
    "            unknown += 1\n",
    "            if y_pred[id_s][id_w] == y_test[id_s][id_w]:\n",
    "                correct_unknown += 1\n",
    "\n",
    "total = correct + correct_unknown + unknown\n",
    "\n",
    "print(\"Total: \", total)\n",
    "print(\"Correct: \", correct)\n",
    "print(\"Acc: \", correct / total)\n",
    "print(\"Unknown: \", unknown)\n",
    "print(\"Unknown acc: \", correct_unknown / unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write result to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50349\n",
      "50349\n"
     ]
    }
   ],
   "source": [
    "f = open(\"TEST_F1.txt\", \"r\")\n",
    "f_res = open(\"TEST_F1_LSTM_BIDIRECTIONAL.txt\", \"w\")\n",
    "\n",
    "content = f.readlines()\n",
    "id_f = 0\n",
    "\n",
    "for id_s, sentence in enumerate(X_test):   \n",
    "    for id_w, word in enumerate(sentence):   \n",
    "        \n",
    "        word_line = str(content[id_f])\n",
    "        \n",
    "        if word_line == '\\n':\n",
    "            f_res.write(\"\\n\")\n",
    "            id_f += 1\n",
    "        \n",
    "        word_line = str(content[id_f])\n",
    "        \n",
    "        # Check if word is in this content line\n",
    "        if word.lower() in word_line.lower():\n",
    "            pred = y_pred[id_s][id_w]\n",
    "            to_file = word_line.strip() + \" \" + str(pred) + \"\\n\"\n",
    "            f_res.write(to_file)\n",
    "            id_f += 1\n",
    "\n",
    "f_res = open(\"TEST_F1_LSTM_BIDIRECTIONAL.txt\", \"r\")         \n",
    "print(len(content))\n",
    "print(len(f_res.readlines()))\n",
    "\n",
    "f.close()\n",
    "f_res.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSIMPLERNN\\n\\nprocessed 46666 tokens with 5648 phrases; found: 6006 phrases; correct: 4143.\\naccuracy:  94.42%; precision:  68.98%; recall:  73.35%; FB1:  71.10\\n              LOC: precision:  75.26%; recall:  85.73%; FB1:  80.16  1900\\n             MISC: precision:  54.47%; recall:  58.97%; FB1:  56.63  760\\n              ORG: precision:  56.19%; recall:  61.47%; FB1:  58.71  1817\\n              PER: precision:  83.58%; recall:  79.04%; FB1:  81.25  1529\\n              \\n\\nLSTM_BIDIRECTIONAL\\n              \\nprocessed 46666 tokens with 5648 phrases; found: 5497 phrases; correct: 4727.\\naccuracy:  96.64%; precision:  85.99%; recall:  83.69%; FB1:  84.83\\n              LOC: precision:  85.76%; recall:  89.93%; FB1:  87.80  1749\\n             MISC: precision:  75.92%; recall:  67.81%; FB1:  71.63  627\\n              ORG: precision:  82.70%; recall:  77.72%; FB1:  80.14  1561\\n              PER: precision:  93.59%; recall:  90.29%; FB1:  91.91  1560\\n\\n'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "SIMPLERNN\n",
    "\n",
    "processed 46666 tokens with 5648 phrases; found: 6006 phrases; correct: 4143.\n",
    "accuracy:  94.42%; precision:  68.98%; recall:  73.35%; FB1:  71.10\n",
    "              LOC: precision:  75.26%; recall:  85.73%; FB1:  80.16  1900\n",
    "             MISC: precision:  54.47%; recall:  58.97%; FB1:  56.63  760\n",
    "              ORG: precision:  56.19%; recall:  61.47%; FB1:  58.71  1817\n",
    "              PER: precision:  83.58%; recall:  79.04%; FB1:  81.25  1529\n",
    "              \n",
    "\n",
    "LSTM_BIDIRECTIONAL\n",
    "              \n",
    "processed 46666 tokens with 5648 phrases; found: 5497 phrases; correct: 4727.\n",
    "accuracy:  96.64%; precision:  85.99%; recall:  83.69%; FB1:  84.83\n",
    "              LOC: precision:  85.76%; recall:  89.93%; FB1:  87.80  1749\n",
    "             MISC: precision:  75.92%; recall:  67.81%; FB1:  71.63  627\n",
    "              ORG: precision:  82.70%; recall:  77.72%; FB1:  80.14  1561\n",
    "              PER: precision:  93.59%; recall:  90.29%; FB1:  91.91  1560\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    # Serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"models/\" + name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    # Serialize weights to HDF5\n",
    "    model.save_weights(\"models/\" + name + \".h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "    json_file = open(\"models/\" + name + \".json\", 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"models/\" + name + \".h5\")\n",
    "    model = loaded_model\n",
    "    print(\"Loaded model from disk\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(model, 'LSTM_F1_8483')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#model = load_model('LSTM_F1_84')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
