{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(13)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(13)\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, MaxPooling3D, Dropout, Embedding, SimpleRNN, LSTM, GRU\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2             \n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '-DOCSTART- -X- -X- O'}]\n",
      "{'id': '-DOCSTART- -X- -X- O'}\n",
      "<class '__main__.Token'>\n",
      "[{'id': 'EU NNP B-NP B-ORG'}, {'id': 'rejects VBZ B-VP O'}, {'id': 'German JJ B-NP B-MISC'}, {'id': 'call NN I-NP O'}, {'id': 'to TO B-VP O'}, {'id': 'boycott VB I-VP O'}, {'id': 'British JJ B-NP B-MISC'}, {'id': 'lamb NN I-NP O'}, {'id': '. . O O'}]\n",
      "La\n",
      "True\n",
      "{'form', 'pos', 'lemma', 'cpos', 'id', 'feats'}\n",
      "{'form', 'lemma', 'pos', 'cpos', 'id', 'feats'}\n",
      "dict_keys(['id', 'form', 'lemma', 'cpos', 'pos', 'feats'])\n",
      "['1', 'da', 'La', 'num=s|gen=f', 'el', 'd']\n",
      "['1', 'da', 'La', 'num=s|gen=f', 'el', 'd']\n",
      "['1', 'da', 'La', 'num=s|gen=f', 'el', 'd']\n",
      "Token value: dict_values(['1', 'La', 'el', 'd', 'da', 'num=s|gen=f'])\n",
      "['1', 'La', 'num=s|gen=f', 'da', 'el', 'd']\n"
     ]
    }
   ],
   "source": [
    "# %load conll_dictorizer.py\n",
    "\"\"\"\n",
    "CoNLL 2009 file readers and writers for the parts of speech.\n",
    "Version with a class modeled as a vectorizer\n",
    "\"\"\"\n",
    "__author__ = \"Pierre Nugues\"\n",
    "\n",
    "import regex as re\n",
    "\n",
    "\n",
    "def save(file, corpus_dict, column_names):\n",
    "    \"\"\"\n",
    "    Saves the corpus in a file\n",
    "    :param file:\n",
    "    :param corpus_dict:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(file, 'w') as f_out:\n",
    "        for sentence in corpus_dict:\n",
    "            sentence_lst = []\n",
    "            for row in sentence:\n",
    "                items = map(lambda x: row.get(x, '_'), column_names)\n",
    "                sentence_lst += '\\t'.join(items) + '\\n'\n",
    "            sentence_lst += '\\n'\n",
    "            f_out.write(''.join(sentence_lst))\n",
    "\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    BASE = os.getcwd()\n",
    "    train_file = os.path.join(BASE, 'datasets/train.txt')\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'cpos', 'pos', 'feats']\n",
    "    train = open(train_file).read().strip()\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "    train_dict = conll_dict.transform(train)\n",
    "\n",
    "    print(train_dict[0])\n",
    "    print(train_dict[0][0])\n",
    "    print(type(train_dict[0][0]))\n",
    "    #print(train_dict[0][0]['form'])\n",
    "    print(train_dict[1])\n",
    "    tok = Token({'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'})\n",
    "    print(tok['form'])\n",
    "    print('form' in tok)\n",
    "\n",
    "    save('out', train_dict, column_names)\n",
    "\n",
    "    tok_dict = {'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'}\n",
    "    tok_dict2 = {'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'}\n",
    "\n",
    "    tok_set = set(tok_dict)\n",
    "    print(tok_set)\n",
    "\n",
    "    tok_set = tok_set.union(tok_dict2)\n",
    "    print(tok_set)\n",
    "\n",
    "    print(tok.keys())\n",
    "\n",
    "    # exit()\n",
    "    word_set = set()\n",
    "    word_set = set(tok_dict.values())\n",
    "    print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    word_set = set(tok.values())\n",
    "    print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    word_set.update(tok.values())\n",
    "    print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    print(\"Token value:\", tok.values())\n",
    "    word_set = word_set.union(set(tok.values()))\n",
    "    print(list(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'form': '-DOCSTART-', 'ppos': '-X-', 'pchunk': '-X-', 'ner': 'O'}]\n",
      "[{'form': 'EU', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-ORG'}, {'form': 'rejects', 'ppos': 'VBZ', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'German', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'call', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'boycott', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'British', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}]\n"
     ]
    }
   ],
   "source": [
    "# %load datasets.py\n",
    "from conll_dictorizer import CoNLLDictorizer, Token\n",
    "import os\n",
    "\n",
    "def load_conll2009_pos():\n",
    "    train_file = 'datasets\\train.txt'\n",
    "    dev_file = 'datasets\\valid.txt'\n",
    "    test_file = 'datasets\\test.txt'\n",
    "    test2_file = 'simple_pos_test.txt'\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'plemma', 'pos', 'ppos']\n",
    "\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "def load_conll2003_en():\n",
    "    BASE_DIR = os.getcwd()\n",
    "    train_file = BASE_DIR + '/datasets/train.txt'\n",
    "    dev_file = BASE_DIR + '/datasets/valid.txt'\n",
    "    test_file = BASE_DIR + '/datasets/test.txt'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "    train_dict = conll_dict.transform(train_sentences)\n",
    "    val_dict = conll_dict.transform(dev_sentences)\n",
    "    test_dict = conll_dict.transform(test_sentences)\n",
    "    print(train_dict[0])\n",
    "    print(train_dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14987"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'form': 'EU', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-ORG'},\n",
       " {'form': 'rejects', 'ppos': 'VBZ', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       " {'form': 'German', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'},\n",
       " {'form': 'call', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       " {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       " {'form': 'boycott', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       " {'form': 'British', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'},\n",
       " {'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       " {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dict is a list of lists of dictionaries\n",
    "def extract_WORDS_NER_TAGS(train_dict):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for list_dict in train_dict: # list of dicts, each list is a sentence\n",
    "        X_sentence = []\n",
    "        Y_sentence = []\n",
    "        for d in list_dict: # each dict is a word\n",
    "            word = d['form'].lower()\n",
    "            ner = d['ner'].lower()\n",
    "            if word != \"-docstart-\":\n",
    "                X_sentence.append(word)\n",
    "                Y_sentence.append(ner)\n",
    "        \n",
    "        X.append(X_sentence)\n",
    "        Y.append(Y_sentence)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices_words(X, glove):\n",
    "    d_indices = {}\n",
    "    # 0 = padding,  1 = unknown\n",
    "    c = 2\n",
    "            \n",
    "    # Add all words from training set       \n",
    "    for sentence in X:\n",
    "        for word in sentence:\n",
    "            if word not in d_indices:\n",
    "                d_indices[word] = c\n",
    "                c += 1\n",
    "            \n",
    "    print(c)\n",
    "    \n",
    "        \n",
    "    # Add all glove words\n",
    "    for word in glove:\n",
    "        if word not in d_indices:\n",
    "            d_indices[word] = c\n",
    "            c += 1\n",
    "        \n",
    "    return d_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices_NER(Y):\n",
    "    d_indices = {}\n",
    "    c = 0\n",
    "    \n",
    "    # Add all NER tags from training set       \n",
    "    for sentence in Y:\n",
    "        for tag in sentence:\n",
    "            if tag not in d_indices:\n",
    "                d_indices[tag] = c\n",
    "                c += 1\n",
    "        \n",
    "    return d_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_indices(d_indices):\n",
    "    return {v: k for k, v in d_indices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_GloVe(file):\n",
    "    embeddings_dict = {}\n",
    "    glove = open(file, encoding='utf-8')\n",
    "    \n",
    "    for line in glove:\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding_vec_word = np.array(line[1:], dtype='float32')\n",
    "        embeddings_dict[word] = embedding_vec_word\n",
    "        \n",
    "    glove.close()\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract WORDS and NER Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = extract_WORDS_NER_TAGS(train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load GloVe vector embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = load_GloVe('glove.6B/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create indices and inverted indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21011\n"
     ]
    }
   ],
   "source": [
    "X_indices = create_indices_words(X, embeddings_dict)\n",
    "Y_indices = create_indices_NER(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_inv_indicies = create_inverted_indices(X_indices)\n",
    "Y_inv_indicies = create_inverted_indices(Y_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate most similar word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_embeddings(embeddings_dict, key_word, n):\n",
    "    \n",
    "    cs_dict = {}\n",
    "    key_word_embedding = embeddings_dict[key_word]\n",
    "    \n",
    "    for word, embedding in embeddings_dict.items():\n",
    "        cs = cosine_similarity([key_word_embedding], [embedding])[0][0]\n",
    "        cs_dict[word] = cs\n",
    "    \n",
    "    cs_sorted = sorted(cs_dict.items(), key=itemgetter(1), reverse=True)[1:n+1] # Do not return table\n",
    "        \n",
    "    return cs_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cs_most_similar = most_similar_embeddings(embeddings_dict, 'table', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cs_most_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create GloVe word embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_GloVe_matrix(X_indices, embeddings_dict):\n",
    "    c = 0\n",
    "    for word in X_indices:\n",
    "        embedding = embeddings_dict.get(word)\n",
    "        if embedding is not None:\n",
    "            word_embedding_matrix[c] = embedding\n",
    "        else:\n",
    "            word_embedding_matrix[c] = np.random.rand(1, 100)\n",
    "        c += 1\n",
    "    return word_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = np.random.rand(len(X_indices), 100)\n",
    "word_embedding_matrix = fill_GloVe_matrix(X_indices, embeddings_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create sequenced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14987"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402594"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b-org'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(X, Y, X_indices, Y_indices):\n",
    "    # Find sequences. Each sequence is seperated by '.'\n",
    "    X_all_seq = []\n",
    "    X_seq = []\n",
    "    Y_all_seq = []\n",
    "    Y_seq = []\n",
    "    \n",
    "    for i_s, sentence in enumerate(X):\n",
    "        for i_w, w in enumerate(sentence):\n",
    "            \n",
    "            word_encdeod = X_indices.get(w)\n",
    "            tag = Y[i_s][i_w]\n",
    "            tag_encoded = Y_indices.get(tag)\n",
    "            #print(w, word_encdeod)\n",
    "            #print(tag, tag_encoded)\n",
    "            \n",
    "            X_seq.append(word_encdeod)\n",
    "            Y_seq.append(tag_encoded)\n",
    "        \n",
    "        X_all_seq.append(np.array(X_seq))\n",
    "        X_seq = []\n",
    "        Y_all_seq.append(np.array(Y_seq))\n",
    "        Y_seq = []\n",
    "\n",
    "    print(len(X_all_seq))\n",
    "    max_len = max(len(l) for l in X_all_seq)\n",
    "    #     avg_len = np.average(np.array([len(x) for x in X_all_seq]))\n",
    "        \n",
    "    X_all_seq = pad_sequences(X_all_seq, 50)\n",
    "    Y_all_seq = pad_sequences(Y_all_seq, 50)\n",
    "    \n",
    "    return np.array(X_all_seq), np.array(Y_all_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14987\n"
     ]
    }
   ],
   "source": [
    "X_seq, Y_seq = build_sequences(X, Y, X_indices, Y_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14987"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_seq.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9004\n",
      "3466\n"
     ]
    }
   ],
   "source": [
    "X_val, Y_val = extract_WORDS_NER_TAGS(val_dict)\n",
    "X_indices_val = create_indices_words(X_val, embeddings_dict)\n",
    "Y_indices_val = create_indices_NER(Y_val)\n",
    "X_inv_indicies_val = create_inverted_indices(X_indices_val)\n",
    "Y_inv_indicies_val = create_inverted_indices(Y_indices_val)\n",
    "X_seq_val, Y_seq_val = build_sequences(X_val, Y_val, X_indices_val, Y_indices_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21010"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_seq.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14987.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(len(X_seq[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21011 = number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_seq[1042]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 50, 10)            210100    \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                550       \n",
      "=================================================================\n",
      "Total params: 210,860\n",
      "Trainable params: 210,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "vocab_size = 21010\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=10, input_length=50))\n",
    "model.add(SimpleRNN(10, activation='relu', return_sequences=False))\n",
    "model.add(Dense(50, activation='softmax'))\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_seq, Y_seq, epochs=5, batch_size=10, verbose=1, validation_data=(X_seq_val, Y_seq_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
