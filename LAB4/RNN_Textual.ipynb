{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(13)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(13)\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, MaxPooling3D, Dropout, Embedding, Bidirectional, SimpleRNN, LSTM, GRU\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json \n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tempfile import TemporaryFile\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '-DOCSTART- -X- -X- O'}]\n",
      "{'id': '-DOCSTART- -X- -X- O'}\n",
      "<class '__main__.Token'>\n",
      "[{'id': 'EU NNP B-NP B-ORG'}, {'id': 'rejects VBZ B-VP O'}, {'id': 'German JJ B-NP B-MISC'}, {'id': 'call NN I-NP O'}, {'id': 'to TO B-VP O'}, {'id': 'boycott VB I-VP O'}, {'id': 'British JJ B-NP B-MISC'}, {'id': 'lamb NN I-NP O'}, {'id': '. . O O'}]\n",
      "La\n",
      "True\n",
      "{'cpos', 'form', 'feats', 'pos', 'id', 'lemma'}\n"
     ]
    }
   ],
   "source": [
    "# %load conll_dictorizer.py\n",
    "\"\"\"\n",
    "CoNLL 2009 file readers and writers for the parts of speech.\n",
    "Version with a class modeled as a vectorizer\n",
    "\"\"\"\n",
    "__author__ = \"Pierre Nugues\"\n",
    "\n",
    "import regex as re\n",
    "\n",
    "\n",
    "def save(file, corpus_dict, column_names):\n",
    "    \"\"\"\n",
    "    Saves the corpus in a file\n",
    "    :param file:\n",
    "    :param corpus_dict:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(file, 'w') as f_out:\n",
    "        for sentence in corpus_dict:\n",
    "            sentence_lst = []\n",
    "            for row in sentence:\n",
    "                items = map(lambda x: row.get(x, '_'), column_names)\n",
    "                sentence_lst += '\\t'.join(items) + '\\n'\n",
    "            sentence_lst += '\\n'\n",
    "            f_out.write(''.join(sentence_lst))\n",
    "\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    BASE = os.getcwd()\n",
    "    train_file = os.path.join(BASE, 'datasets/train.txt')\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'cpos', 'pos', 'feats']\n",
    "    train = open(train_file).read().strip()\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "    train_dict = conll_dict.transform(train)\n",
    "\n",
    "    print(train_dict[0])\n",
    "    print(train_dict[0][0])\n",
    "    print(type(train_dict[0][0]))\n",
    "    #print(train_dict[0][0]['form'])\n",
    "    print(train_dict[1])\n",
    "    tok = Token({'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'})\n",
    "    print(tok['form'])\n",
    "    print('form' in tok)\n",
    "\n",
    "    save('out', train_dict, column_names)\n",
    "\n",
    "    tok_dict = {'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'}\n",
    "    tok_dict2 = {'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'}\n",
    "\n",
    "    tok_set = set(tok_dict)\n",
    "    print(tok_set)\n",
    "\n",
    "    tok_set = tok_set.union(tok_dict2)\n",
    "    #print(tok_set)\n",
    "\n",
    "    #print(tok.keys())\n",
    "\n",
    "    # exit()\n",
    "    word_set = set()\n",
    "    word_set = set(tok_dict.values())\n",
    "    #print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    word_set = set(tok.values())\n",
    "    #print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    word_set.update(tok.values())\n",
    "    #print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    #print(\"Token value:\", tok.values())\n",
    "    word_set = word_set.union(set(tok.values()))\n",
    "    #print(list(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'form': '-DOCSTART-', 'ppos': '-X-', 'pchunk': '-X-', 'ner': 'O'}]\n",
      "[{'form': 'EU', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-ORG'}, {'form': 'rejects', 'ppos': 'VBZ', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'German', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'call', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'boycott', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'British', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}]\n"
     ]
    }
   ],
   "source": [
    "# %load datasets.py\n",
    "from conll_dictorizer import CoNLLDictorizer, Token\n",
    "import os\n",
    "\n",
    "def load_conll2009_pos():\n",
    "    train_file = 'datasets\\train.txt'\n",
    "    dev_file = 'datasets\\valid.txt'\n",
    "    test_file = 'datasets\\test.txt'\n",
    "    test2_file = 'simple_pos_test.txt'\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'plemma', 'pos', 'ppos']\n",
    "\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "def load_conll2003_en():\n",
    "    BASE_DIR = os.getcwd()\n",
    "    train_file = BASE_DIR + '/datasets/train.txt'\n",
    "    dev_file = BASE_DIR + '/datasets/valid.txt'\n",
    "    test_file = BASE_DIR + '/datasets/test.txt'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "    train_dict = conll_dict.transform(train_sentences)\n",
    "    val_dict = conll_dict.transform(dev_sentences)\n",
    "    test_dict = conll_dict.transform(test_sentences)\n",
    "    print(train_dict[0])\n",
    "    print(train_dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(file):\n",
    "    embeddings_dict = {}\n",
    "    glove = open(file, encoding='utf-8')\n",
    "    \n",
    "    for line in glove:\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding_vec_word = np.array(line[1:], dtype='float32')\n",
    "        embeddings_dict[word] = embedding_vec_word\n",
    "        \n",
    "    glove.close()\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_name):\n",
    "    with open('files/' + file_name + '.pkl', 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(file_name, file):\n",
    "    with open('files/' + file_name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = load_file('embeddings_dict')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = load_file('X')\n",
    "y = load_file('y')\n",
    "X_indices_to_words = load_file('X_indices_to_words')\n",
    "y_indices_to_len = load_file('y_indices_to_len')\n",
    "X_words_to_indices = load_file('X_words_to_indices')\n",
    "y_len_to_indices = load_file('y_len_to_indices')\n",
    "X_vocabulary = load_file('X_vocabulary')\n",
    "y_vocabulary = load_file('X_vocabulary')\n",
    "X_only_indices = load_file('X_only_indices')\n",
    "y_only_indices = load_file('y_only_indices')\n",
    "word_embedding_matrix = load_file('word_embedding_matrix')\n",
    "embeddings_dict = load_file('embeddings_dict')\n",
    "cs_most_similar = load_file('cs_most_similar')\n",
    "X_train = load_file('X_train')\n",
    "y_train = load_file('y_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_dict is a list of lists of dictionaries\n",
    "def extract_features(train_dict):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for sentence in train_dict:\n",
    "        X_sentence = []\n",
    "        y_sentence = []\n",
    "        for word in sentence:\n",
    "            w = word['form'].lower()\n",
    "            n = word['ner']\n",
    "            X_sentence.append(w)\n",
    "            y_sentence.append(n)\n",
    "    \n",
    "        X.append(X_sentence)\n",
    "        y.append(y_sentence)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract words and ner tags - X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence words:  ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "Sentence NER:  ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "X, y = extract_features(train_dict)\n",
    "print('Sentence words: ', X[1])\n",
    "print('Sentence NER: ', y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(X, WORDS=True):\n",
    "    X_vocabulary = set()\n",
    "    \n",
    "    if WORDS:\n",
    "        X_vocabulary.add(\"UNKNOWN_WORD\")\n",
    "        \n",
    "    for sentence in X:\n",
    "        for word in sentence:\n",
    "            X_vocabulary.add(word)\n",
    "    \n",
    "    return sorted(list(X_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size WORDS:  21011\n"
     ]
    }
   ],
   "source": [
    "X_vocabulary = create_vocabulary(X, WORDS=True)\n",
    "print(\"Vocabulary size WORDS: \", len(X_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size NER:  9\n"
     ]
    }
   ],
   "source": [
    "y_vocabulary = create_vocabulary(y, WORDS=False)\n",
    "print(\"Vocabulary size NER: \", len(y_vocabulary))\n",
    "nbr_of_classes = len(y_vocabulary) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add words from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in the vocabulary total, X_vocabulary: 402596\n"
     ]
    }
   ],
   "source": [
    "for word in embeddings_dict.keys():\n",
    "    X_vocabulary.append(word)\n",
    "\n",
    "X_vocabulary = sorted(list(set(X_vocabulary)))\n",
    "total_word_count = len(X_vocabulary)\n",
    "print('Words in the vocabulary total, X_vocabulary:', total_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create indices and inverted indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices(X):\n",
    "    return dict(enumerate(X), start=2) # 0 is padding, 1 is unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "X_indices_to_words = {}\n",
    "\n",
    "for w in X_vocabulary:\n",
    "    X_indices_to_words[i] = w\n",
    "    i += 1\n",
    "\n",
    "y_indices_to_len = {}\n",
    "y_indices_to_len[0] = 'O' #PADDING\n",
    "y_indices_to_len[1] = 'UNKNOWN_WORD'\n",
    "i = 2\n",
    "\n",
    "for l in y_vocabulary:\n",
    "    if l != 'O':\n",
    "        y_indices_to_len[i] = l\n",
    "        i += 1\n",
    "        \n",
    "#X_indices_to_len = dict(enumerate(X_vocabulary), start=2)\n",
    "#y_indices_to_len = dict(enumerate(y_vocabulary), start=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_indices(X):\n",
    "    return {v: k for k, v in X.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words_to_indices = create_inverted_indices(X_indices_to_words)\n",
    "y_len_to_indices = create_inverted_indices(y_indices_to_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index: [('!', 2), ('!!', 3), ('!!!', 4)]\n",
      "LEN index: [('O', 0), ('UNKNOWN_WORD', 1), ('B-LOC', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Word index:', list(X_words_to_indices.items())[:3])\n",
    "print('LEN index:', list(y_len_to_indices.items())[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode lists - Convert to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_to_indices(X, X_words_to_indices, num_words=None):\n",
    "    X_encoded = []\n",
    "    for x in X:\n",
    "        X_encoded_words = []\n",
    "        if num_words:\n",
    "            # We map the unknown words to the second first index of the matrix, for the test set\n",
    "            X_encoded_words = list(map(lambda x: X_words_to_indices.get(x,1), x))\n",
    "        else:\n",
    "             X_encoded_words = list(map(X_words_to_indices.get, x))\n",
    "            #for val in x:\n",
    "            #    X_encoded_words.append(X_words_to_indices.get(val))\n",
    "        X_encoded += [X_encoded_words]\n",
    "    return X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentences, word indices [359699, 143138, 107474, 318005, 271940, 361488, 195554, 126463, 391264, 161837, 48420, 363369, 109493, 363369, 332754, 85853, 218261, 375629, 324031, 123767, 389005, 231734, 112304, 126757, 92049, 72526, 366525, 363369, 330316, 936]\n",
      "\n",
      "First sentences, LEN indices [0, 4, 8, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "X_only_indices = encode_to_indices(X, X_words_to_indices)\n",
    "y_only_indices = encode_to_indices(y, y_len_to_indices)\n",
    "print('First sentences, word indices', X_only_indices[4])\n",
    "print(\"\")\n",
    "print('First sentences, LEN indices', y_only_indices[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_embeddings(embeddings_dict, key_word, n):\n",
    "    \n",
    "    d = {}\n",
    "    key_word_embedding = embeddings_dict[key_word]\n",
    "    \n",
    "    for word, embedding in embeddings_dict.items():\n",
    "        d_val = cosine_similarity([key_word_embedding], [embedding])[0][0]\n",
    "        d[word] = d_val\n",
    "    \n",
    "    d_sorted = sorted(d.items(), key=itemgetter(1), reverse=True)[1:n+1] # Do not return table\n",
    "        \n",
    "    return d_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table = most_similar_embeddings(embeddings_dict, 'table', 5)\n",
    "#france = most_similar_embeddings(embeddings_dict, 'france', 5)\n",
    "#sweden = most_similar_embeddings(embeddings_dict, 'sweden', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tables', 0.8021162), ('place', 0.6582378), ('bottom', 0.65597194), ('room', 0.6543691), ('side', 0.6433667)]\n",
      "[('belgium', 0.8076422), ('french', 0.80043775), ('britain', 0.79505277), ('spain', 0.75574636), ('paris', 0.7481586)]\n",
      "[('denmark', 0.86244005), ('norway', 0.807325), ('finland', 0.79064953), ('netherlands', 0.74684644), ('austria', 0.7466836)]\n"
     ]
    }
   ],
   "source": [
    "print(table)\n",
    "print(france)\n",
    "print(sweden)\n",
    "\n",
    "'''\n",
    "\n",
    "[('tables', 0.8021162), ('place', 0.6582378), ('bottom', 0.65597194), ('room', 0.6543691), ('side', 0.6433667)]\n",
    "[('belgium', 0.8076422), ('french', 0.80043775), ('britain', 0.79505277), ('spain', 0.75574636), ('paris', 0.7481586)]\n",
    "[('denmark', 0.86244005), ('norway', 0.807325), ('finland', 0.79064953), ('netherlands', 0.74684644), ('austria', 0.7466836)]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_glove_matrix(X_vocabulary, embeddings_dict):\n",
    "    for word in X_vocabulary:\n",
    "        if word in embeddings_dict:\n",
    "            i = X_words_to_indices[word]\n",
    "            embedding = embeddings_dict[word]\n",
    "            word_embedding_matrix[i] = embedding\n",
    "    return word_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (402598, 100)\n"
     ]
    }
   ],
   "source": [
    "word_embedding_matrix = np.random.random((len(X_vocabulary)+2, 100))\n",
    "word_embedding_matrix = fill_glove_matrix(X_vocabulary, embeddings_dict)\n",
    "print('Shape of embedding matrix:', word_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the longest sequence in either train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length in train:  113\n",
      "Maximum sentence length in val:  109\n",
      "Maximum sentence length in test:  124\n",
      "Maximum sentence length total:  124\n"
     ]
    }
   ],
   "source": [
    "max_seq_len_train = max(len(s) for s in X)\n",
    "print(\"Maximum sentence length in train: \", max_seq_len_train)\n",
    "\n",
    "X_val, _ = extract_features(val_dict)\n",
    "max_seq_len_val = max(len(s) for s in X_val)\n",
    "print(\"Maximum sentence length in val: \", max_seq_len_val)\n",
    "\n",
    "X_test, _ = extract_features(test_dict)\n",
    "max_seq_len_test = max(len(s) for s in X_test)\n",
    "print(\"Maximum sentence length in test: \", max_seq_len_test)\n",
    "\n",
    "max_seq_len = max(max_seq_len_train, max_seq_len_val, max_seq_len_test)\n",
    "\n",
    "print(\"Maximum sentence length total: \", max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_only_indices, maxlen=max_seq_len)\n",
    "y_train = pad_sequences(y_only_indices, maxlen=max_seq_len)\n",
    "\n",
    "# The number of classes and 0 (padding symbol)\n",
    "y_train = to_categorical(y_train, num_classes=nbr_of_classes + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0 113352    679 221876 354361 275585  63472 364506\n",
      "  49151 192164 381012    936]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 4 0 0 0 0 0 0 0 0]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X_val, y_val = extract_features(val_dict)\n",
    "\n",
    "X_only_indices_val = encode_to_indices(X_val, X_words_to_indices, num_words=total_word_count)\n",
    "y_only_indices_val = encode_to_indices(y_val, y_len_to_indices)\n",
    "\n",
    "X_val = pad_sequences(X_only_indices_val, maxlen=max_seq_len)\n",
    "y_val = pad_sequences(y_only_indices_val, maxlen=max_seq_len)\n",
    "\n",
    "y_val = to_categorical(y_val, num_classes=nbr_of_classes + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIMPLE RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simpleRNN():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_word_count+2,\n",
    "                               100,\n",
    "                               mask_zero=True,\n",
    "                               input_length=max_seq_len))\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    model.layers[0].trainable = True\n",
    "    model.add(SimpleRNN(32, return_sequences=True))\n",
    "    model.add(Dense(nbr_of_classes + 1, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_word_count+2,\n",
    "                               100,\n",
    "                               mask_zero=True,\n",
    "                               input_length=max_seq_len))\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    model.layers[0].trainable = True\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dense(nbr_of_classes + 1, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM BIDIRECTIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM_BIDIRECTIONAL():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_word_count+2,\n",
    "                               100,\n",
    "                               mask_zero=True,\n",
    "                               input_length=max_seq_len))\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    model.layers[0].trainable = True\n",
    "    model.add(SimpleRNN(512, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "    model.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "    model.add(Dense(nbr_of_classes + 1, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_LSTM_BIDIRECTIONAL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 124, 100)          40259800  \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 124, 512)          313856    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 124, 512)          1574912   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 124, 12)           6156      \n",
      "=================================================================\n",
      "Total params: 42,154,724\n",
      "Trainable params: 42,154,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=2),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_acc', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/1\n",
      "14987/14987 [==============================] - 169s 11ms/step - loss: 0.0243 - acc: 0.9917 - val_loss: 0.1126 - val_acc: 0.9765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11f22f11550>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=callbacks, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = extract_features(test_dict)\n",
    "\n",
    "# We create the parallel sequences of indexes\n",
    "X_only_indices_test = encode_to_indices(X_test, X_words_to_indices, num_words=total_word_count)\n",
    "y_only_indices_test = encode_to_indices(y_test, y_len_to_indices)\n",
    "\n",
    "X_test_padded = pad_sequences(X_only_indices_test, maxlen=max_seq_len)\n",
    "y_test_padded = pad_sequences(y_only_indices_test, maxlen=max_seq_len)\n",
    "\n",
    "# The number of LEN classes and 0 (padding symbol)\n",
    "y_test_vectorized = to_categorical(y_test_padded, num_classes=nbr_of_classes + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684 3684 3684 3684\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test), len(y_test), len(X_test_padded), len(y_test_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 16s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test_padded, y_test_vectorized, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.15566251565268452\n",
      "Accuracy:  0.9671301056714839\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = model.predict(X_test_padded)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('X_test', X_test[1])\n",
    "print('X_test_padded', X_test_padded[1])\n",
    "print('Y_test', y_test[1])\n",
    "print('Y_test_padded', y_test_padded[1])\n",
    "print('predictions', y_pred_probs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove padding and extract pred with highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 1.02456221e-11 3.63981485e-08 1.38617118e-08\n",
      "  1.21875043e-08 2.77253935e-08 6.79346357e-10 1.05724824e-08\n",
      "  5.76844184e-10 7.01748548e-10 4.52988255e-12 4.47230274e-12]]\n"
     ]
    }
   ],
   "source": [
    "# Remove padding\n",
    "y_pred_probs_no_padd = []\n",
    "for sent_nbr, sent_len_predictions in enumerate(y_pred_probs):\n",
    "    y_pred_probs_no_padd += [sent_len_predictions[-len(X_test[sent_nbr]):]]\n",
    "print(y_pred_probs_no_padd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O'], ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O'], ['B-PER', 'I-PER']]\n",
      "\n",
      "[['O'], ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O'], ['B-PER', 'I-PER']]\n"
     ]
    }
   ],
   "source": [
    "# Extract prediction with highest probability and convert indices to symbols\n",
    "y_pred = []\n",
    "for sentence in y_pred_probs_no_padd:\n",
    "    len_idx = list(map(np.argmax, sentence))\n",
    "    len_cat = list(map(y_indices_to_len.get, len_idx))\n",
    "    y_pred += [len_cat]\n",
    "\n",
    "print(y_pred[:3])\n",
    "print(\"\")\n",
    "print(y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total;  47274\n",
      "Correct:  45155\n",
      "Acc:  0.9551762067944325\n",
      "Unknown:  1143\n",
      "Unknown acc:  0.8538932633420823\n"
     ]
    }
   ],
   "source": [
    "total, correct, unknown, correct_unknown = 0, 0, 0, 0\n",
    "\n",
    "for id_s, sentence in enumerate(X_test):\n",
    "    for id_w, word in enumerate(sentence):\n",
    "        if y_pred[id_s][id_w] == y_test[id_s][id_w]:\n",
    "            correct += 1\n",
    "            \n",
    "        # The word is not in the dictionary\n",
    "        if word not in X_words_to_indices:\n",
    "            unknown += 1\n",
    "            if y_pred[id_s][id_w] == y_test[id_s][id_w]:\n",
    "                correct_unknown += 1\n",
    "\n",
    "total = correct + correct_unknown + unknown\n",
    "\n",
    "#confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Total; \", total)\n",
    "print(\"Correct: \", correct)\n",
    "print(\"Acc: \", correct / total)\n",
    "print(\"Unknown: \", unknown)\n",
    "print(\"Unknown acc: \", correct_unknown / unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write result to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50349\n",
      "50349\n"
     ]
    }
   ],
   "source": [
    "f = open(\"test_f1.txt\", \"r\")\n",
    "f_res = open(\"test_f1_res_12.txt\", \"w\")\n",
    "\n",
    "content = f.readlines()\n",
    "id_f = 0\n",
    "\n",
    "for id_s, sentence in enumerate(X_test):   \n",
    "    for id_w, word in enumerate(sentence):   \n",
    "        \n",
    "        word_line = str(content[id_f])\n",
    "        \n",
    "        if word_line == '\\n':\n",
    "            f_res.write(\"\\n\")\n",
    "            id_f += 1\n",
    "        \n",
    "        word_line = str(content[id_f])\n",
    "        \n",
    "        # Check if word is in this content line\n",
    "        if word.lower() in word_line.lower():\n",
    "            pred = y_pred[id_s][id_w]\n",
    "            to_file = word_line.strip() + \" \" + str(pred) + \"\\n\"\n",
    "            f_res.write(to_file)\n",
    "            id_f += 1\n",
    "\n",
    "f_res = open(\"test_f1_res_12.txt\", \"r\")         \n",
    "print(len(content))\n",
    "print(len(f_res.readlines()))\n",
    "\n",
    "f.close()\n",
    "f_res.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    # Serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"models/\" + name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    # Serialize weights to HDF5\n",
    "    model.save_weights(\"models/\" + name + \".h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "    json_file = open(\"models/\" + name + \".json\", 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"models/\" + name + \".h5\")\n",
    "    model = loaded_model\n",
    "    print(\"Loaded model from disk\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#save_model(model, 'x') # NODES, EPOCHS, BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#model = load_model('LSTM_F1_84')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
