{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(13)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(13)\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, MaxPooling3D, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2             \n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    "Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a \n",
    "subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text into \n",
    "pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, \n",
    "quantities, monetary values, percentages, etc.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'datasets\\test.txt', sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '-DOCSTART- -X- -X- O'}]\n",
      "{'id': '-DOCSTART- -X- -X- O'}\n",
      "<class '__main__.Token'>\n",
      "[{'id': 'EU NNP B-NP B-ORG'}, {'id': 'rejects VBZ B-VP O'}, {'id': 'German JJ B-NP B-MISC'}, {'id': 'call NN I-NP O'}, {'id': 'to TO B-VP O'}, {'id': 'boycott VB I-VP O'}, {'id': 'British JJ B-NP B-MISC'}, {'id': 'lamb NN I-NP O'}, {'id': '. . O O'}]\n",
      "La\n",
      "True\n",
      "{'cpos', 'pos', 'feats', 'form', 'id', 'lemma'}\n",
      "{'pos', 'id', 'lemma', 'cpos', 'feats', 'form'}\n",
      "dict_keys(['id', 'form', 'lemma', 'cpos', 'pos', 'feats'])\n",
      "['el', 'La', '1', 'num=s|gen=f', 'da', 'd']\n",
      "['el', 'La', '1', 'num=s|gen=f', 'da', 'd']\n",
      "['el', 'La', '1', 'num=s|gen=f', 'da', 'd']\n",
      "Token value: dict_values(['1', 'La', 'el', 'd', 'da', 'num=s|gen=f'])\n",
      "['d', 'el', 'La', '1', 'num=s|gen=f', 'da']\n"
     ]
    }
   ],
   "source": [
    "# %load conll_dictorizer.py\n",
    "\"\"\"\n",
    "CoNLL 2009 file readers and writers for the parts of speech.\n",
    "Version with a class modeled as a vectorizer\n",
    "\"\"\"\n",
    "__author__ = \"Pierre Nugues\"\n",
    "\n",
    "import regex as re\n",
    "\n",
    "\n",
    "def save(file, corpus_dict, column_names):\n",
    "    \"\"\"\n",
    "    Saves the corpus in a file\n",
    "    :param file:\n",
    "    :param corpus_dict:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(file, 'w') as f_out:\n",
    "        for sentence in corpus_dict:\n",
    "            sentence_lst = []\n",
    "            for row in sentence:\n",
    "                items = map(lambda x: row.get(x, '_'), column_names)\n",
    "                sentence_lst += '\\t'.join(items) + '\\n'\n",
    "            sentence_lst += '\\n'\n",
    "            f_out.write(''.join(sentence_lst))\n",
    "\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    BASE = os.getcwd()\n",
    "    train_file = os.path.join(BASE, 'datasets/train.txt')\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'cpos', 'pos', 'feats']\n",
    "    train = open(train_file).read().strip()\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "    train_dict = conll_dict.transform(train)\n",
    "\n",
    "    print(train_dict[0])\n",
    "    print(train_dict[0][0])\n",
    "    print(type(train_dict[0][0]))\n",
    "    #print(train_dict[0][0]['form'])\n",
    "    print(train_dict[1])\n",
    "    tok = Token({'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'})\n",
    "    print(tok['form'])\n",
    "    print('form' in tok)\n",
    "\n",
    "    save('out', train_dict, column_names)\n",
    "\n",
    "    tok_dict = {'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'}\n",
    "    tok_dict2 = {'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'}\n",
    "\n",
    "    tok_set = set(tok_dict)\n",
    "    print(tok_set)\n",
    "\n",
    "    tok_set = tok_set.union(tok_dict2)\n",
    "    print(tok_set)\n",
    "\n",
    "    print(tok.keys())\n",
    "\n",
    "    # exit()\n",
    "    word_set = set()\n",
    "    word_set = set(tok_dict.values())\n",
    "    print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    word_set = set(tok.values())\n",
    "    print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    word_set.update(tok.values())\n",
    "    print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    print(\"Token value:\", tok.values())\n",
    "    word_set = word_set.union(set(tok.values()))\n",
    "    print(list(word_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'form': '-DOCSTART-', 'ppos': '-X-', 'pchunk': '-X-', 'ner': 'O'}]\n",
      "[{'form': 'EU', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-ORG'}, {'form': 'rejects', 'ppos': 'VBZ', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'German', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'call', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'boycott', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'British', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}]\n"
     ]
    }
   ],
   "source": [
    "# %load datasets.py\n",
    "from conll_dictorizer import CoNLLDictorizer, Token\n",
    "import os\n",
    "\n",
    "def load_conll2009_pos():\n",
    "    train_file = r'datasets/train.txt'\n",
    "    dev_file = 'datasets/valid.txt'\n",
    "    test_file = 'datasets/test.txt'\n",
    "    #test2_file = 'simple_pos_test.txt'\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'plemma', 'pos', 'ppos']\n",
    "\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "def load_conll2003_en():\n",
    "    BASE_DIR = os.getcwd()\n",
    "    train_file = BASE_DIR + '/datasets/train.txt'\n",
    "    dev_file = BASE_DIR + '/datasets/valid.txt'\n",
    "    test_file = BASE_DIR + '/datasets/test.txt'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "    train_dict = conll_dict.transform(train_sentences)\n",
    "    print(train_dict[0])\n",
    "    print(train_dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': '-DOCSTART-', 'ppos': '-X-', 'pchunk': '-X-', 'ner': 'O'}],\n",
       " [{'form': 'EU', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-ORG'},\n",
       "  {'form': 'rejects', 'ppos': 'VBZ', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'German', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'},\n",
       "  {'form': 'call', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'boycott', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'British', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'},\n",
       "  {'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}],\n",
       " [{'form': 'Peter', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-PER'},\n",
       "  {'form': 'Blackburn', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'I-PER'}],\n",
       " [{'form': 'BRUSSELS', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-LOC'},\n",
       "  {'form': '1996-08-22', 'ppos': 'CD', 'pchunk': 'I-NP', 'ner': 'O'}],\n",
       " [{'form': 'The', 'ppos': 'DT', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'European', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'B-ORG'},\n",
       "  {'form': 'Commission', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'I-ORG'},\n",
       "  {'form': 'said', 'ppos': 'VBD', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'on', 'ppos': 'IN', 'pchunk': 'B-PP', 'ner': 'O'},\n",
       "  {'form': 'Thursday', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'it', 'ppos': 'PRP', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'disagreed', 'ppos': 'VBD', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'with', 'ppos': 'IN', 'pchunk': 'B-PP', 'ner': 'O'},\n",
       "  {'form': 'German', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'},\n",
       "  {'form': 'advice', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-PP', 'ner': 'O'},\n",
       "  {'form': 'consumers', 'ppos': 'NNS', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'shun', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'British', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'},\n",
       "  {'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'until', 'ppos': 'IN', 'pchunk': 'B-SBAR', 'ner': 'O'},\n",
       "  {'form': 'scientists', 'ppos': 'NNS', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'determine', 'ppos': 'VBP', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'whether', 'ppos': 'IN', 'pchunk': 'B-SBAR', 'ner': 'O'},\n",
       "  {'form': 'mad', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'cow', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'disease', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'can', 'ppos': 'MD', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'be', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'transmitted', 'ppos': 'VBN', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-PP', 'ner': 'O'},\n",
       "  {'form': 'sheep', 'ppos': 'NN', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}],\n",
       " [{'form': 'Germany', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-LOC'},\n",
       "  {'form': \"'s\", 'ppos': 'POS', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'representative', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-PP', 'ner': 'O'},\n",
       "  {'form': 'the', 'ppos': 'DT', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'European', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'B-ORG'},\n",
       "  {'form': 'Union', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'I-ORG'},\n",
       "  {'form': \"'s\", 'ppos': 'POS', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'veterinary', 'ppos': 'JJ', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'committee', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'Werner', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'B-PER'},\n",
       "  {'form': 'Zwingmann', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'I-PER'},\n",
       "  {'form': 'said', 'ppos': 'VBD', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'on', 'ppos': 'IN', 'pchunk': 'B-PP', 'ner': 'O'},\n",
       "  {'form': 'Wednesday', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'consumers', 'ppos': 'NNS', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'should', 'ppos': 'MD', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'buy', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'sheepmeat', 'ppos': 'NN', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'from', 'ppos': 'IN', 'pchunk': 'B-PP', 'ner': 'O'},\n",
       "  {'form': 'countries', 'ppos': 'NNS', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'other', 'ppos': 'JJ', 'pchunk': 'B-ADJP', 'ner': 'O'},\n",
       "  {'form': 'than', 'ppos': 'IN', 'pchunk': 'B-PP', 'ner': 'O'},\n",
       "  {'form': 'Britain', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-LOC'},\n",
       "  {'form': 'until', 'ppos': 'IN', 'pchunk': 'B-SBAR', 'ner': 'O'},\n",
       "  {'form': 'the', 'ppos': 'DT', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'scientific', 'ppos': 'JJ', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'advice', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'was', 'ppos': 'VBD', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'clearer', 'ppos': 'JJR', 'pchunk': 'B-ADJP', 'ner': 'O'},\n",
       "  {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}],\n",
       " [{'form': '\"', 'ppos': '\"', 'pchunk': 'O', 'ner': 'O'},\n",
       "  {'form': 'We', 'ppos': 'PRP', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'do', 'ppos': 'VBP', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': \"n't\", 'ppos': 'RB', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'support', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'any', 'ppos': 'DT', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'such', 'ppos': 'JJ', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'recommendation', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'because', 'ppos': 'IN', 'pchunk': 'B-SBAR', 'ner': 'O'},\n",
       "  {'form': 'we', 'ppos': 'PRP', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'do', 'ppos': 'VBP', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': \"n't\", 'ppos': 'RB', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'see', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'any', 'ppos': 'DT', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'grounds', 'ppos': 'NNS', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'for', 'ppos': 'IN', 'pchunk': 'B-PP', 'ner': 'O'},\n",
       "  {'form': 'it', 'ppos': 'PRP', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': ',', 'ppos': ',', 'pchunk': 'O', 'ner': 'O'},\n",
       "  {'form': '\"', 'ppos': '\"', 'pchunk': 'O', 'ner': 'O'},\n",
       "  {'form': 'the', 'ppos': 'DT', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'Commission', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'B-ORG'},\n",
       "  {'form': \"'s\", 'ppos': 'POS', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'chief', 'ppos': 'JJ', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'spokesman', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'Nikolaus', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'B-PER'},\n",
       "  {'form': 'van', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'I-PER'},\n",
       "  {'form': 'der', 'ppos': 'FW', 'pchunk': 'I-NP', 'ner': 'I-PER'},\n",
       "  {'form': 'Pas', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'I-PER'},\n",
       "  {'form': 'told', 'ppos': 'VBD', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'a', 'ppos': 'DT', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'news', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'briefing', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}],\n",
       " [{'form': 'He', 'ppos': 'PRP', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'said', 'ppos': 'VBD', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'further', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'scientific', 'ppos': 'JJ', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'study', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'was', 'ppos': 'VBD', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'required', 'ppos': 'VBN', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'and', 'ppos': 'CC', 'pchunk': 'O', 'ner': 'O'},\n",
       "  {'form': 'if', 'ppos': 'IN', 'pchunk': 'B-SBAR', 'ner': 'O'},\n",
       "  {'form': 'it', 'ppos': 'PRP', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'was', 'ppos': 'VBD', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'found', 'ppos': 'VBN', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'that', 'ppos': 'IN', 'pchunk': 'B-SBAR', 'ner': 'O'},\n",
       "  {'form': 'action', 'ppos': 'NN', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'was', 'ppos': 'VBD', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'needed', 'ppos': 'VBN', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'it', 'ppos': 'PRP', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'should', 'ppos': 'MD', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'be', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'taken', 'ppos': 'VBN', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'by', 'ppos': 'IN', 'pchunk': 'B-PP', 'ner': 'O'},\n",
       "  {'form': 'the', 'ppos': 'DT', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'European', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'B-ORG'},\n",
       "  {'form': 'Union', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'I-ORG'},\n",
       "  {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}],\n",
       " [{'form': 'He', 'ppos': 'PRP', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'said', 'ppos': 'VBD', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'a', 'ppos': 'DT', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'proposal', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'last', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'month', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'by', 'ppos': 'IN', 'pchunk': 'B-PP', 'ner': 'O'},\n",
       "  {'form': 'EU', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-ORG'},\n",
       "  {'form': 'Farm', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'Commissioner', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'Franz', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'B-PER'},\n",
       "  {'form': 'Fischler', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'I-PER'},\n",
       "  {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'ban', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'sheep', 'ppos': 'NN', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'brains', 'ppos': 'NNS', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': ',', 'ppos': ',', 'pchunk': 'O', 'ner': 'O'},\n",
       "  {'form': 'spleens', 'ppos': 'NNS', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'and', 'ppos': 'CC', 'pchunk': 'O', 'ner': 'O'},\n",
       "  {'form': 'spinal', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'cords', 'ppos': 'NNS', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'from', 'ppos': 'IN', 'pchunk': 'B-PP', 'ner': 'O'},\n",
       "  {'form': 'the', 'ppos': 'DT', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'human', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'and', 'ppos': 'CC', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'animal', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'food', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'chains', 'ppos': 'NNS', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'was', 'ppos': 'VBD', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'a', 'ppos': 'DT', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'highly', 'ppos': 'RB', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'specific', 'ppos': 'JJ', 'pchunk': 'B-ADJP', 'ner': 'O'},\n",
       "  {'form': 'and', 'ppos': 'CC', 'pchunk': 'I-ADJP', 'ner': 'O'},\n",
       "  {'form': 'precautionary', 'ppos': 'JJ', 'pchunk': 'I-ADJP', 'ner': 'O'},\n",
       "  {'form': 'move', 'ppos': 'NN', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'protect', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'human', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'health', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}],\n",
       " [{'form': 'Fischler', 'ppos': 'JJR', 'pchunk': 'B-NP', 'ner': 'B-PER'},\n",
       "  {'form': 'proposed', 'ppos': 'VBN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'EU-wide', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'B-MISC'},\n",
       "  {'form': 'measures', 'ppos': 'VBZ', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'after', 'ppos': 'IN', 'pchunk': 'B-PP', 'ner': 'O'},\n",
       "  {'form': 'reports', 'ppos': 'NNS', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'from', 'ppos': 'IN', 'pchunk': 'B-PP', 'ner': 'O'},\n",
       "  {'form': 'Britain', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-LOC'},\n",
       "  {'form': 'and', 'ppos': 'CC', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'France', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'B-LOC'},\n",
       "  {'form': 'that', 'ppos': 'WDT', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'under', 'ppos': 'IN', 'pchunk': 'B-PP', 'ner': 'O'},\n",
       "  {'form': 'laboratory', 'ppos': 'NN', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'conditions', 'ppos': 'NNS', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'sheep', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'could', 'ppos': 'MD', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'contract', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'Bovine', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-MISC'},\n",
       "  {'form': 'Spongiform', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'I-MISC'},\n",
       "  {'form': 'Encephalopathy', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'I-MISC'},\n",
       "  {'form': '(', 'ppos': '(', 'pchunk': 'O', 'ner': 'O'},\n",
       "  {'form': 'BSE', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-MISC'},\n",
       "  {'form': ')', 'ppos': ')', 'pchunk': 'O', 'ner': 'O'},\n",
       "  {'form': '--', 'ppos': ':', 'pchunk': 'O', 'ner': 'O'},\n",
       "  {'form': 'mad', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'O'},\n",
       "  {'form': 'cow', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'disease', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dict is a list of lists of dictionaries\n",
    "def extract_WORDS_NER_TAGS(train_dict):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for list_dict in train_dict: # list of dicts\n",
    "        for d in list_dict:\n",
    "            word = d['form'].lower()\n",
    "            ner = d['ner'].lower()\n",
    "            if word != \"-docstart-\":\n",
    "                X.append(word)\n",
    "                Y.append(ner)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices_words(X, glove):\n",
    "    d_indices = {}\n",
    "    # 0 = padding,  1 = unknown\n",
    "    c = 2\n",
    "    \n",
    "    # Add all glove words\n",
    "    for word in glove:\n",
    "        if word not in d_indices:\n",
    "            d_indices[word] = c\n",
    "            c += 1\n",
    "            \n",
    "    # Add all words from training set       \n",
    "    for word in X:\n",
    "        if word not in d_indices:\n",
    "            d_indices[word] = c\n",
    "            c += 1\n",
    "        \n",
    "    return d_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices_NER(Y):\n",
    "    d_indices = {}\n",
    "    c = 0\n",
    "    \n",
    "    # Add all NER tags from training set       \n",
    "    for tag in Y:\n",
    "        if tag not in d_indices:\n",
    "            d_indices[tag] = c\n",
    "            c += 1\n",
    "        \n",
    "    return d_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_indices(d_indices):\n",
    "    return {v: k for k, v in d_indices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_GloVe(file):\n",
    "    embeddings_dict = {}\n",
    "    glove = open(file, encoding='utf-8')\n",
    "    \n",
    "    for line in glove:\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding_vec_word = np.array(line[1:], dtype='float32')\n",
    "        embeddings_dict[word] = embedding_vec_word\n",
    "        \n",
    "    glove.close()\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract WORDS and NER Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = extract_WORDS_NER_TAGS(train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load GloVe vector embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = load_GloVe('glove.6B/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create indices and inverted indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_indices = create_indices_words(X, embeddings_dict)\n",
    "Y_indices = create_indices_NER(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_inv_indicies = create_inverted_indices(X_indices)\n",
    "Y_inv_indicies = create_inverted_indices(Y_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate most similar word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_embeddings(embeddings_dict, key_word, n):\n",
    "    \n",
    "    cs_dict = {}\n",
    "    key_word_embedding = embeddings_dict[key_word]\n",
    "    \n",
    "    for word, embedding in embeddings_dict.items():\n",
    "        cs = cosine_similarity([key_word_embedding], [embedding])[0][0]\n",
    "        cs_dict[word] = cs\n",
    "    \n",
    "    cs_sorted = sorted(cs_dict.items(), key=itemgetter(1), reverse=True)[1:n+1] # Do not return table\n",
    "        \n",
    "    return cs_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_most_similar = most_similar_embeddings(embeddings_dict, 'table', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tables', 0.8021162),\n",
       " ('place', 0.6582378),\n",
       " ('bottom', 0.65597194),\n",
       " ('room', 0.6543691),\n",
       " ('side', 0.6433667)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_most_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create GloVe word embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_GloVe_matrix(X_indices, embeddings_dict):\n",
    "    c = 0\n",
    "    for word in X_indices:\n",
    "        embedding = embeddings_dict.get(word)\n",
    "        if embedding is not None:\n",
    "            word_embedding_matrix[c] = embedding\n",
    "        else:\n",
    "            word_embedding_matrix[c] = np.random.rand(1, 100)\n",
    "        c += 1\n",
    "    return word_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = np.random.rand(len(X_indices), 100)\n",
    "word_embedding_matrix = fill_GloVe_matrix(X_indices, embeddings_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create sequenced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(X, Y, X_indices, Y_indices):\n",
    "    # Find sequences. Each sequence is seperated by '.'\n",
    "    X_all_seq = []\n",
    "    X_seq = []\n",
    "    Y_all_seq = []\n",
    "    Y_seq = []\n",
    "    \n",
    "    for i, w in enumerate(X):\n",
    "        X_seq.append(X_indices.get(w))\n",
    "        Y_seq.append(Y_indices.get(Y[i]))\n",
    "\n",
    "        if w == '.':\n",
    "            X_all_seq.append(X_seq)\n",
    "            X_seq = []\n",
    "            Y_all_seq.append(Y_seq)\n",
    "            Y_seq = []\n",
    "\n",
    "    max_len = max(len(l) for l in X_all_seq)\n",
    "        \n",
    "    #X_all_seq = np.array(np.array_split(X_all_seq, max_len))\n",
    "    X_all_seq = pad_sequences(X_all_seq, max_len)\n",
    "    Y_all_seq = pad_sequences(Y_all_seq, max_len)\n",
    "    \n",
    "    return X_all_seq, Y_all_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq, Y_seq = build_sequences(X, Y, X_indices, Y_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
